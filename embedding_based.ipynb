{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FC1zPK9NamYN",
        "ZJ1-_8tG8YBt",
        "dDYFGE-JsMuT"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Sampling of Blood Smear Images"
      ],
      "metadata": {
        "id": "4UcIJj79qpwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataloader"
      ],
      "metadata": {
        "id": "FC1zPK9NamYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75s1YnBzFJWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def resize_and_pad(image, target_size=(224, 224), fill=0):\n",
        "    \"\"\"\n",
        "    Resize an image to fit within the target size while preserving the aspect ratio,\n",
        "    then pad the shorter sides with a constant value to reach the target size.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): Input image.\n",
        "        target_size (tuple): Target size (width, height), default (224, 224).\n",
        "        fill (int or tuple): Padding fill value (e.g., 0 for black, 255 for white).\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: Resized and padded image of size target_size.\n",
        "    \"\"\"\n",
        "    # Get original dimensions\n",
        "    width, height = image.size\n",
        "    target_width, target_height = target_size\n",
        "\n",
        "    # Calculate scaling factor to fit within target size\n",
        "    scale = min(target_width / width, target_height / height)\n",
        "    new_width = int(width * scale)\n",
        "    new_height = int(height * scale)\n",
        "\n",
        "    # Resize image with correct (height, width) order\n",
        "    resized_image = transforms.functional.resize(image, (new_height, new_width))\n",
        "\n",
        "    # Calculate padding to center the image\n",
        "    padding_left = (target_width - new_width) // 2\n",
        "    padding_top = (target_height - new_height) // 2\n",
        "    # Ensure padding adds up exactly to the difference\n",
        "    padding_right = target_width - new_width - padding_left\n",
        "    padding_bottom = target_height - new_height - padding_top\n",
        "\n",
        "    # Apply padding\n",
        "    padded_image = transforms.functional.pad(\n",
        "        resized_image,\n",
        "        padding=(padding_left, padding_top, padding_right, padding_bottom),\n",
        "        fill=fill\n",
        "    )\n",
        "\n",
        "    return padded_image"
      ],
      "metadata": {
        "id": "fByqZWp5vcNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Image transformation\n",
        "def get_base_transforms(target_size=(320, 320), use_grayscale=False):\n",
        "    \"\"\"\n",
        "    Returns a composed set of basic image transformations for preprocessing input images.\n",
        "\n",
        "    Parameters:\n",
        "    - target_size (tuple): The desired output size (height, width) of the image after resizing and padding.\n",
        "    - use_grayscale (bool): If True, converts the image to grayscale with 3 channels before applying other transformations.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Optional grayscale conversion with 3 output channels,\n",
        "        - Resizing and padding the image to match the target size,\n",
        "        - Conversion to tensor,\n",
        "        - Normalization using ImageNet mean and standard deviation.\n",
        "    \"\"\"\n",
        "    base_transforms = [\n",
        "        transforms.Lambda(lambda img: resize_and_pad(img, target_size=target_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        "\n",
        "    if use_grayscale:\n",
        "        base_transforms.insert(0, transforms.Grayscale(num_output_channels=3))  # Keep 3 channels for compatibility\n",
        "    return transforms.Compose(base_transforms)\n",
        "\n",
        "\n",
        "# Data augmentation transforms\n",
        "def get_augmentation_transforms():\n",
        "    \"\"\"\n",
        "    Returns a composed set of data augmentation transformations to artificially expand the training dataset.\n",
        "\n",
        "    This function applies a series of random transformations to simulate variations in brightness, contrast, orientation,\n",
        "    and color mode, helping the model generalize better.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Random brightness and contrast adjustment (ColorJitter),\n",
        "        - Random horizontal and vertical flipping,\n",
        "        - Random rotation by up to Â±10 degrees,\n",
        "        - Random conversion to grayscale with a 20% probability.\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Simulate lighting/stain variations\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomGrayscale(p=0.2)  # Optional: Randomly apply grayscale as part of augmentation\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "osiwpi1mvvjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, num_ways=5, num_support=5,\n",
        "                 num_query=10, num_episodes=100, target_size=(224, 224),\n",
        "                 use_grayscale=False,\n",
        "                 augment=False,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Path to dataset directory\n",
        "            split (str): One of 'train', 'validation', or 'test'\n",
        "            num_ways (int): Number of classes per episode\n",
        "            num_support (int): Number of support samples per class (i.e. number of shots)\n",
        "            num_query (int): Number of query samples per class\n",
        "            num_episodes (int): Number of episodes per epoch\n",
        "            use_grayscale(bool),  Use grayscale or not\n",
        "            augment(bool),        For data augmentation technique\n",
        "        \"\"\"\n",
        "        self.split_dir = os.path.join(data_dir, split)\n",
        "        self.num_ways = num_ways\n",
        "        self.num_support = num_support\n",
        "        self.num_query = num_query\n",
        "        self.num_episodes = num_episodes\n",
        "        base_transform = get_base_transforms(target_size, use_grayscale)\n",
        "        if augment:\n",
        "            augmentation_transform = get_augmentation_transforms()\n",
        "            self.transform = transforms.Compose([augmentation_transform, base_transform])\n",
        "        else:\n",
        "            self.transform = base_transform\n",
        "\n",
        "        # Load class directories and their images\n",
        "        self.classes = [c for c in os.listdir(self.split_dir)\n",
        "                       if os.path.isdir(os.path.join(self.split_dir, c))]\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}  # Map class names to indices\n",
        "        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}  # Map indices to class names\n",
        "        self.class_images = {\n",
        "            c: [os.path.join(self.split_dir, c, img)\n",
        "                for img in os.listdir(os.path.join(self.split_dir, c))]\n",
        "            for c in self.classes\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_episodes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Randomly select N classes for this episode\n",
        "        selected_classes = np.random.choice(self.classes, self.num_ways, replace=False)\n",
        "\n",
        "        support_images = []\n",
        "        support_labels = []\n",
        "        query_images = []\n",
        "        query_labels = []\n",
        "\n",
        "        for label_idx, class_name in enumerate(selected_classes):\n",
        "            all_images = self.class_images[class_name]\n",
        "            if len(all_images) < self.num_support + self.num_query:\n",
        "                raise ValueError(\n",
        "                    f\"Class {class_name} has only {len(all_images)} images. \"\n",
        "                    f\"Need at least {self.num_support + self.num_query}.\"\n",
        "                )\n",
        "\n",
        "            # Randomly select support and query images\n",
        "            selected_indices = np.random.choice(\n",
        "                len(all_images),\n",
        "                self.num_support + self.num_query,\n",
        "                replace=False #True#########################################################################\n",
        "            )\n",
        "            support_paths = [all_images[i] for i in selected_indices[:self.num_support]]\n",
        "            query_paths = [all_images[i] for i in selected_indices[self.num_support:]]\n",
        "\n",
        "            # Load and transform support images\n",
        "            for path in support_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                support_images.append(img)\n",
        "                support_labels.append(label_idx)\n",
        "\n",
        "            # Load and transform query images\n",
        "            for path in query_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                query_images.append(img)\n",
        "                query_labels.append(label_idx)\n",
        "\n",
        "        # Shuffle the support and query sets\n",
        "        support_indices = np.arange(len(support_images))\n",
        "        query_indices = np.arange(len(query_images))\n",
        "        np.random.shuffle(support_indices)\n",
        "        np.random.shuffle(query_indices)\n",
        "\n",
        "        support_images = [support_images[i] for i in support_indices]\n",
        "        support_labels = [support_labels[i] for i in support_indices]\n",
        "        query_images = [query_images[i] for i in query_indices]\n",
        "        query_labels = [query_labels[i] for i in query_indices]\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        support_set = (\n",
        "            torch.stack(support_images),\n",
        "            torch.tensor(support_labels, dtype=torch.long)\n",
        "        )\n",
        "        query_set = (\n",
        "            torch.stack(query_images),\n",
        "            torch.tensor(query_labels, dtype=torch.long)\n",
        "        )\n",
        "        # Store the selected class names for this episode\n",
        "        selected_classes = [str(cls) for cls in selected_classes]\n",
        "        episode_classes = selected_classes\n",
        "\n",
        "        return support_set, query_set, episode_classes"
      ],
      "metadata": {
        "id": "YQtpp1OmFV25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loader(dataset, batch_size=1, shuffle=True):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the dataset.\n",
        "    Note: Batch size should typically be 1 for few-shot learning,\n",
        "    as each episode is a separate task.\n",
        "    \"\"\"\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )"
      ],
      "metadata": {
        "id": "paGubE7bFSLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "tJNGAZFtcFtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Based"
      ],
      "metadata": {
        "id": "5-XVthB2sSH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Drive"
      ],
      "metadata": {
        "id": "LjC8q7C_-4Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Adjust data_dir based on your Drive structure\n",
        "data_dir = '/content/drive/MyDrive/Computer vision with few shot sampling focus group/data_set'\n",
        "output_dir = '/content/drive/MyDrive/Computer vision with few shot sampling focus group/protonet_results' # For saving models/plots"
      ],
      "metadata": {
        "id": "fJwE5uZm-257"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(query_embeddings, prototype_embeddings):\n",
        "    \"\"\"\n",
        "    Calculates the Euclidean distance between query embeddings and prototypes.\n",
        "    Args:\n",
        "        query_embeddings (Tensor): shape (num_query_total, embedding_dim)\n",
        "        prototype_embeddings (Tensor): shape (num_ways, embedding_dim)\n",
        "    Returns:\n",
        "        distances (Tensor): shape (num_query_total, num_ways)\n",
        "    \"\"\"\n",
        "    num_query_total = query_embeddings.size(0)\n",
        "    num_ways = prototype_embeddings.size(0)\n",
        "    # Expand dimensions for broadcasting:\n",
        "    # query: (num_query_total, 1, embedding_dim)\n",
        "    # proto: (1, num_ways, embedding_dim)\n",
        "    distances = (query_embeddings.unsqueeze(1) - prototype_embeddings.unsqueeze(0)).pow(2).sum(dim=2)\n",
        "    # Result shape: (num_query_total, num_ways)\n",
        "    return distances\n",
        "\n",
        "class PrototypicalNetwork(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            encoder (nn.Module): The feature extractor network.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def calculate_prototypes(self, support_embeddings, support_labels, num_ways):\n",
        "        \"\"\"\n",
        "        Calculates class prototypes from support set embeddings.\n",
        "        Args:\n",
        "            support_embeddings (Tensor): Embeddings of the support set (num_support_total, embedding_dim).\n",
        "            support_labels (Tensor): Labels for the support set (num_support_total).\n",
        "            num_ways (int): Number of classes in the episode.\n",
        "        Returns:\n",
        "            prototypes (Tensor): Class prototypes (num_ways, embedding_dim).\n",
        "        \"\"\"\n",
        "        embedding_dim = support_embeddings.size(-1)\n",
        "        prototypes = torch.zeros(num_ways, embedding_dim, device=support_embeddings.device)\n",
        "        for i in range(num_ways):\n",
        "            class_mask = (support_labels == i)\n",
        "            class_embeddings = support_embeddings[class_mask]\n",
        "            if class_embeddings.shape[0] > 0: # Handle cases where a class might have 0 examples (shouldn't happen with good data)\n",
        "                 prototypes[i] = class_embeddings.mean(dim=0)\n",
        "            else:\n",
        "                 # Optional: Handle this case e.g., with a small random vector or zero vector\n",
        "                 print(f\"Warning: Class {i} had 0 support examples in prototype calculation.\")\n",
        "                 prototypes[i] = torch.zeros(embedding_dim, device=support_embeddings.device)\n",
        "        return prototypes\n",
        "\n",
        "    def forward(self, support_images, support_labels, query_images):\n",
        "        \"\"\"\n",
        "        Processes an episode (support and query sets) to produce logits for query samples.\n",
        "        Args:\n",
        "            support_images (Tensor): Images of the support set (num_support_total, C, H, W).\n",
        "            support_labels (Tensor): Labels for the support set (num_support_total).\n",
        "            query_images (Tensor): Images of the query set (num_query_total, C, H, W).\n",
        "        Returns:\n",
        "            logits (Tensor): Negative distances (logits) for query classification (num_query_total, num_ways).\n",
        "        \"\"\"\n",
        "        num_support_total = support_images.size(0)\n",
        "        num_query_total = query_images.size(0)\n",
        "        num_ways = int(support_labels.max()) + 1 # Infer num_ways from labels\n",
        "\n",
        "        # Concatenate support and query images for efficient encoding\n",
        "        all_images = torch.cat([support_images, query_images], dim=0)\n",
        "        all_embeddings = self.encoder(all_images)\n",
        "\n",
        "        # Split embeddings back into support and query\n",
        "        support_embeddings = all_embeddings[:num_support_total]\n",
        "        query_embeddings = all_embeddings[num_support_total:]\n",
        "\n",
        "        # Calculate prototypes\n",
        "        prototypes = self.calculate_prototypes(support_embeddings, support_labels, num_ways)\n",
        "\n",
        "        # Calculate distances between query embeddings and prototypes\n",
        "        distances = euclidean_distance(query_embeddings, prototypes)\n",
        "\n",
        "        # Return negative distances as logits (closer distance = higher probability)\n",
        "        return -distances\n",
        "\n",
        "# --- Accuracy Helper ---\n",
        "def calculate_accuracy(logits, targets):\n",
        "    \"\"\"Calculates classification accuracy.\"\"\"\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "    accuracy = (predictions == targets).float().mean()\n",
        "    return accuracy.item()"
      ],
      "metadata": {
        "id": "mKGNQMhysXTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training and Evaluation Functions ---\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"Runs a single training epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    # Use tqdm for progress bar\n",
        "    with tqdm(dataloader, desc=\"Training\", leave=False) as pbar:\n",
        "        for batch in pbar:\n",
        "            # Data loader yields episodes one by one (batch_size=1 typically)\n",
        "            # Squeeze the batch dimension (index 0)\n",
        "            support_set, query_set, _ = batch # Ignore episode_classes for training\n",
        "            support_images, support_labels = support_set\n",
        "            query_images, query_labels = query_set\n",
        "\n",
        "            # Remove the extra batch dimension of 1\n",
        "            support_images = support_images.squeeze(0).to(device)\n",
        "            support_labels = support_labels.squeeze(0).to(device)\n",
        "            query_images = query_images.squeeze(0).to(device)\n",
        "            query_labels = query_labels.squeeze(0).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(support_images, support_labels, query_images)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(logits, query_labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate accuracy for the episode\n",
        "            accuracy = calculate_accuracy(logits, query_labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += accuracy\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update progress bar description\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{accuracy:.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
        "    avg_accuracy = total_accuracy / num_batches if num_batches > 0 else 0\n",
        "    return avg_loss, avg_accuracy\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, description=\"Evaluating\"):\n",
        "    \"\"\"Evaluates the model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        # Use tqdm for progress bar\n",
        "        with tqdm(dataloader, desc=description, leave=False) as pbar:\n",
        "            for batch in pbar:\n",
        "                support_set, query_set, _ = batch\n",
        "                support_images, support_labels = support_set\n",
        "                query_images, query_labels = query_set\n",
        "\n",
        "                # Remove the extra batch dimension of 1\n",
        "                support_images = support_images.squeeze(0).to(device)\n",
        "                support_labels = support_labels.squeeze(0).to(device)\n",
        "                query_images = query_images.squeeze(0).to(device)\n",
        "                query_labels = query_labels.squeeze(0).to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = model(support_images, support_labels, query_images)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = criterion(logits, query_labels)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                accuracy = calculate_accuracy(logits, query_labels)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_accuracy += accuracy\n",
        "                num_batches += 1\n",
        "\n",
        "                # Update progress bar description\n",
        "                pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
        "    avg_accuracy = total_accuracy / num_batches if num_batches > 0 else 0\n",
        "    return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "EObESp5LTd78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Script ---\n",
        "if __name__ == '__main__':\n",
        "    # --- Configuration ---\n",
        "    # Google Drive Mount (ensure this runs in Colab)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    best_model_path = os.path.join(output_dir, 'best_protonet_model.pth')\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Few-Shot Learning Parameters\n",
        "    TRAIN_NUM_WAYS = 4          # N-way for training\n",
        "    TRAIN_NUM_SUPPORT = 5       # K-shot for training support set\n",
        "    TRAIN_NUM_QUERY = 15        # Number of query examples per class during training\n",
        "    TRAIN_EPISODES = 200        # Number of training episodes per epoch\n",
        "    VAL_NUM_WAYS = 4            # N-way for validation\n",
        "    VAL_NUM_SUPPORT = 5         # K-shot for validation (can match training or be different)\n",
        "    VAL_NUM_QUERY = 15        # Query examples for validation\n",
        "    VAL_EPISODES = 100         # Number of validation episodes\n",
        "    TEST_NUM_WAYS = 4           # N-way for testing\n",
        "    TEST_NUM_SHOTS = [1, 5, 10] # K-shot values to evaluate on test set\n",
        "    TEST_NUM_QUERY = 15         # Query examples for testing\n",
        "    TEST_EPISODES = 600         # Number of test episodes (higher for stable results, following ProtoNet paper)\n",
        "\n",
        "    # Model & Training Parameters\n",
        "    IMAGE_SIZE = (224, 224) # Target image size (match ResNet input if using standard)\n",
        "    USE_GRAYSCALE = False   # Use grayscale images?\n",
        "    LEARNING_RATE = 1e-4\n",
        "    NUM_EPOCHS = 50\n",
        "    PATIENCE = 10 # Early stopping patience\n",
        "\n",
        "    # --- Encoder Setup ---\n",
        "    # Using a pre-trained ResNet18 as the encoder\n",
        "    encoder = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    # Modify the final layer - we need features, not classification\n",
        "    encoder.fc = nn.Identity() # Output features from the layer before fc\n",
        "\n",
        "    # --- Model, Optimizer, Criterion ---\n",
        "    model = PrototypicalNetwork(encoder).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss() # Standard loss for classification\n",
        "\n",
        "    # --- Datasets and DataLoaders ---\n",
        "    print(\"Setting up datasets...\")\n",
        "    # Training Set\n",
        "    train_dataset = FewShotDataset(\n",
        "        data_dir=data_dir,\n",
        "        split='train',\n",
        "        num_ways=TRAIN_NUM_WAYS,\n",
        "        num_support=TRAIN_NUM_SUPPORT,\n",
        "        num_query=TRAIN_NUM_QUERY,\n",
        "        num_episodes=TRAIN_EPISODES,\n",
        "        target_size=IMAGE_SIZE,\n",
        "        use_grayscale=USE_GRAYSCALE,\n",
        "        augment=True # Enable augmentation for training\n",
        "    )\n",
        "    train_loader = get_data_loader(train_dataset, shuffle=True)\n",
        "    print(f\"Train dataset: {len(train_dataset.classes)} classes found.\")\n",
        "\n",
        "    # Validation Set (using fixed settings, e.g., 5-way 5-shot)\n",
        "    val_dataset = FewShotDataset(\n",
        "        data_dir=data_dir,\n",
        "        split='test', # Make sure you have a 'validation' split folder\n",
        "        num_ways=VAL_NUM_WAYS,\n",
        "        num_support=VAL_NUM_SUPPORT,\n",
        "        num_query=VAL_NUM_QUERY,\n",
        "        num_episodes=VAL_EPISODES,\n",
        "        target_size=IMAGE_SIZE,\n",
        "        use_grayscale=USE_GRAYSCALE,\n",
        "        augment=False # No augmentation for validation/testing\n",
        "    )\n",
        "    val_loader = get_data_loader(val_dataset, shuffle=False) # No need to shuffle validation\n",
        "    print(f\"Validation dataset: {len(val_dataset.classes)} classes found.\")\n",
        "\n",
        "    # Test datasets will be created later inside the evaluation loop for different shots\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    best_val_accuracy = 0.0\n",
        "    epochs_without_improvement = 0\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "        # Train one epoch\n",
        "        train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_accuracy)\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss, val_accuracy = evaluate(model, val_loader, criterion, device, description=\"Validating\")\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_accuracy)\n",
        "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Check for improvement and save best model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            print(f\"Validation accuracy improved ({best_val_accuracy:.4f} -> {val_accuracy:.4f}). Saving model...\")\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            print(f\"No improvement in validation accuracy for {epochs_without_improvement} epochs.\")\n",
        "\n",
        "        # Early stopping\n",
        "        if epochs_without_improvement >= PATIENCE:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n--- Training Finished ---\")\n",
        "\n",
        "    # Plotting training history\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.title('Accuracy History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(output_dir, 'training_history.png')\n",
        "    plt.savefig(plot_path)\n",
        "    print(f\"Training history plot saved to {plot_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # --- Final Evaluation on Test Set ---\n",
        "    print(\"\\n--- Evaluating on Test Set ---\")\n",
        "    if os.path.exists(best_model_path):\n",
        "        print(f\"Loading best model from {best_model_path}\")\n",
        "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    else:\n",
        "        print(\"Warning: No best model found. Evaluating with the last model state.\")\n",
        "\n",
        "    # Test for each specified number of shots\n",
        "    for num_shots in TEST_NUM_SHOTS:\n",
        "        print(f\"\\n--- Evaluating {TEST_NUM_WAYS}-way {num_shots}-shot ---\")\n",
        "        test_dataset = FewShotDataset(\n",
        "            data_dir=data_dir,\n",
        "            split='test', # Make sure you have a 'test' split folder\n",
        "            num_ways=TEST_NUM_WAYS,\n",
        "            num_support=num_shots, # Use the current num_shots for support\n",
        "            num_query=TEST_NUM_QUERY,\n",
        "            num_episodes=TEST_EPISODES,\n",
        "            target_size=IMAGE_SIZE,\n",
        "            use_grayscale=USE_GRAYSCALE,\n",
        "            augment=False # No augmentation\n",
        "        )\n",
        "        if not test_dataset.classes:\n",
        "             print(f\"Skipping {num_shots}-shot evaluation: No classes found or insufficient data in 'test' split.\")\n",
        "             continue\n",
        "\n",
        "        test_loader = get_data_loader(test_dataset, shuffle=False)\n",
        "        print(f\"Test dataset ({num_shots}-shot): {len(test_dataset.classes)} classes found.\")\n",
        "\n",
        "        test_loss, test_accuracy = evaluate(model, test_loader, criterion, device, description=f\"Testing {num_shots}-shot\")\n",
        "        print(f\"Test Results ({num_shots}-shot):\")\n",
        "        print(f\"  Loss: {test_loss:.4f}\")\n",
        "        print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Evaluation Complete ---\")"
      ],
      "metadata": {
        "id": "MZCf60zETu8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}