{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UcIJj79qpwG"
      },
      "source": [
        "# Few Shot Sampling of Blood Smear Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC1zPK9NamYN"
      },
      "source": [
        "## Custom Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "75s1YnBzFJWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fByqZWp5vcNG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def resize_and_pad(image, target_size=(320, 320), fill=0):\n",
        "    \"\"\"\n",
        "    Resize an image to fit within the target size while preserving the aspect ratio,\n",
        "    then pad the shorter sides with a constant value to reach the target size.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): Input image.\n",
        "        target_size (tuple): Target size (width, height), default (224, 224).\n",
        "        fill (int or tuple): Padding fill value (e.g., 0 for black, 255 for white).\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: Resized and padded image of size target_size.\n",
        "    \"\"\"\n",
        "    # Get original dimensions\n",
        "    width, height = image.size\n",
        "    target_width, target_height = target_size\n",
        "\n",
        "    # Calculate scaling factor to fit within target size\n",
        "    scale = min(target_width / width, target_height / height)\n",
        "    new_width = int(width * scale)\n",
        "    new_height = int(height * scale)\n",
        "\n",
        "    # Resize image with correct (height, width) order\n",
        "    resized_image = transforms.functional.resize(image, (new_height, new_width))\n",
        "\n",
        "    # Calculate padding to center the image\n",
        "    padding_left = (target_width - new_width) // 2\n",
        "    padding_top = (target_height - new_height) // 2\n",
        "    # Ensure padding adds up exactly to the difference\n",
        "    padding_right = target_width - new_width - padding_left\n",
        "    padding_bottom = target_height - new_height - padding_top\n",
        "\n",
        "    # Apply padding\n",
        "    padded_image = transforms.functional.pad(\n",
        "        resized_image,\n",
        "        padding=(padding_left, padding_top, padding_right, padding_bottom),\n",
        "        fill=fill\n",
        "    )\n",
        "\n",
        "    return padded_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "osiwpi1mvvjG"
      },
      "outputs": [],
      "source": [
        "# Basic Image transformation\n",
        "def get_base_transforms(target_size=(320, 320), use_grayscale=False):\n",
        "    \"\"\"\n",
        "    Returns a composed set of basic image transformations for preprocessing input images.\n",
        "\n",
        "    Parameters:\n",
        "    - target_size (tuple): The desired output size (height, width) of the image after resizing and padding.\n",
        "    - use_grayscale (bool): If True, converts the image to grayscale with 3 channels before applying other transformations.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Optional grayscale conversion with 3 output channels,\n",
        "        - Resizing and padding the image to match the target size,\n",
        "        - Conversion to tensor,\n",
        "        - Normalization using ImageNet mean and standard deviation.\n",
        "    \"\"\"\n",
        "    base_transforms = [\n",
        "        transforms.Lambda(lambda img: resize_and_pad(img, target_size=target_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        "\n",
        "    if use_grayscale:\n",
        "        base_transforms.insert(0, transforms.Grayscale(num_output_channels=3))  # Keep 3 channels for compatibility\n",
        "    return transforms.Compose(base_transforms)\n",
        "\n",
        "\n",
        "# Data augmentation transforms\n",
        "def get_augmentation_transforms():\n",
        "    \"\"\"\n",
        "    Returns a composed set of data augmentation transformations to artificially expand the training dataset.\n",
        "\n",
        "    This function applies a series of random transformations to simulate variations in brightness, contrast, orientation,\n",
        "    and color mode, helping the model generalize better.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Random brightness and contrast adjustment (ColorJitter),\n",
        "        - Random horizontal and vertical flipping,\n",
        "        - Random rotation by up to ±10 degrees,\n",
        "        - Random conversion to grayscale with a 20% probability.\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Simulate lighting/stain variations\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomGrayscale(p=0.2)  # Optional: Randomly apply grayscale as part of augmentation\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YQtpp1OmFV25"
      },
      "outputs": [],
      "source": [
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, num_ways=5, num_support=5,\n",
        "                 num_query=10, num_episodes=100, target_size=(320, 320),\n",
        "                 use_grayscale=False,\n",
        "                 augment=False,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Path to dataset directory\n",
        "            split (str): One of 'train', 'validation', or 'test'\n",
        "            num_ways (int): Number of classes per episode\n",
        "            num_support (int): Number of support samples per class (i.e. number of shots)\n",
        "            num_query (int): Number of query samples per class\n",
        "            num_episodes (int): Number of episodes per epoch\n",
        "            use_grayscale(bool),  Use grayscale or not\n",
        "            augment(bool),        For data augmentation technique\n",
        "        \"\"\"\n",
        "        self.split_dir = os.path.join(data_dir, split)\n",
        "        self.num_ways = num_ways\n",
        "        self.num_support = num_support\n",
        "        self.num_query = num_query\n",
        "        self.num_episodes = num_episodes\n",
        "        base_transform = get_base_transforms(target_size, use_grayscale)\n",
        "        if augment:\n",
        "            augmentation_transform = get_augmentation_transforms()\n",
        "            self.transform = transforms.Compose([augmentation_transform, base_transform])\n",
        "        else:\n",
        "            self.transform = base_transform\n",
        "\n",
        "        # Load class directories and their images\n",
        "        self.classes = [c for c in os.listdir(self.split_dir)\n",
        "                       if os.path.isdir(os.path.join(self.split_dir, c))]\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}  # Map class names to indices\n",
        "        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}  # Map indices to class names\n",
        "        self.class_images = {\n",
        "            c: [os.path.join(self.split_dir, c, img)\n",
        "                for img in os.listdir(os.path.join(self.split_dir, c))]\n",
        "            for c in self.classes\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_episodes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Randomly select N classes for this episode\n",
        "        selected_classes = np.random.choice(self.classes, self.num_ways, replace=False)\n",
        "\n",
        "        support_images = []\n",
        "        support_labels = []\n",
        "        query_images = []\n",
        "        query_labels = []\n",
        "\n",
        "        for label_idx, class_name in enumerate(selected_classes):\n",
        "            all_images = self.class_images[class_name]\n",
        "            if len(all_images) < self.num_support + self.num_query:\n",
        "                raise ValueError(\n",
        "                    f\"Class {class_name} has only {len(all_images)} images. \"\n",
        "                    f\"Need at least {self.num_support + self.num_query}.\"\n",
        "                )\n",
        "\n",
        "            # Randomly select support and query images\n",
        "            selected_indices = np.random.choice(\n",
        "                len(all_images),\n",
        "                self.num_support + self.num_query,\n",
        "                replace=False #True#########################################################################\n",
        "            )\n",
        "            support_paths = [all_images[i] for i in selected_indices[:self.num_support]]\n",
        "            query_paths = [all_images[i] for i in selected_indices[self.num_support:]]\n",
        "\n",
        "            # Load and transform support images\n",
        "            for path in support_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                support_images.append(img)\n",
        "                support_labels.append(label_idx)\n",
        "\n",
        "            # Load and transform query images\n",
        "            for path in query_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                query_images.append(img)\n",
        "                query_labels.append(label_idx)\n",
        "\n",
        "        # Shuffle the support and query sets\n",
        "        support_indices = np.arange(len(support_images))\n",
        "        query_indices = np.arange(len(query_images))\n",
        "        np.random.shuffle(support_indices)\n",
        "        np.random.shuffle(query_indices)\n",
        "\n",
        "        support_images = [support_images[i] for i in support_indices]\n",
        "        support_labels = [support_labels[i] for i in support_indices]\n",
        "        query_images = [query_images[i] for i in query_indices]\n",
        "        query_labels = [query_labels[i] for i in query_indices]\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        support_set = (\n",
        "            torch.stack(support_images),\n",
        "            torch.tensor(support_labels, dtype=torch.long)\n",
        "        )\n",
        "        query_set = (\n",
        "            torch.stack(query_images),\n",
        "            torch.tensor(query_labels, dtype=torch.long)\n",
        "        )\n",
        "        # Store the selected class names for this episode\n",
        "        selected_classes = [str(cls) for cls in selected_classes]\n",
        "        episode_classes = selected_classes\n",
        "\n",
        "        return support_set, query_set, episode_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "paGubE7bFSLp"
      },
      "outputs": [],
      "source": [
        "def get_data_loader(dataset, batch_size=1, shuffle=True):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the dataset.\n",
        "    Note: Batch size should typically be 1 for few-shot learning,\n",
        "    as each episode is a separate task.\n",
        "    \"\"\"\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJNGAZFtcFtR"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDYFGE-JsMuT"
      },
      "source": [
        "### Optimization Based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z6p-LDjqGfej",
        "outputId": "4f398f6f-cb00-4237-fafc-16a6dd09ec0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install higher --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5CO06RyqsTdZ"
      },
      "outputs": [],
      "source": [
        "# Using the MAML technique\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import higher\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zl3VYu-m7gb8"
      },
      "outputs": [],
      "source": [
        "class MAMLResNet(nn.Module):\n",
        "    def __init__(self, num_ways=5, pretrained=True):\n",
        "        super().__init__()\n",
        "        # Load pretrained ResNet50 and replace the final layer\n",
        "        self.resnet = models.resnet50(pretrained=pretrained)\n",
        "\n",
        "        # Unfreeze only the last few layers for fine-tuning\n",
        "        for param in list(self.resnet.parameters())[:-10]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace the final fully connected layer\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_ways)\n",
        "\n",
        "        # Initialize the final layer with appropriate weights\n",
        "        nn.init.xavier_uniform_(self.resnet.fc.weight)\n",
        "        nn.init.zeros_(self.resnet.fc.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_maml_epoch(\n",
        "    model,\n",
        "    train_loader,\n",
        "    meta_optimizer,\n",
        "    epoch,\n",
        "    num_inner_steps=5,  # Increased inner steps for better adaptation\n",
        "    inner_lr=0.01,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Use tqdm for progress tracking\n",
        "    with tqdm(train_loader, desc=f'Epoch {epoch}') as pbar:\n",
        "        for episode_idx, (support, query, classes) in enumerate(pbar):\n",
        "            # Unpack data and remove batch dimension\n",
        "            support_images, support_labels = support\n",
        "            query_images, query_labels = query\n",
        "\n",
        "            support_images = support_images.squeeze(0).to(device)\n",
        "            support_labels = support_labels.squeeze(0).to(device)\n",
        "            query_images = query_images.squeeze(0).to(device)\n",
        "            query_labels = query_labels.squeeze(0).to(device)\n",
        "\n",
        "            meta_optimizer.zero_grad()\n",
        "\n",
        "            # Use higher library for differentiation through optimization\n",
        "            with higher.innerloop_ctx(model, torch.optim.SGD(model.parameters(), lr=inner_lr),\n",
        "                                     copy_initial_weights=True) as (fast_model, inner_optimizer):\n",
        "                # Inner loop adaptation - multiple steps\n",
        "                for _ in range(num_inner_steps):\n",
        "                    support_outputs = fast_model(support_images)\n",
        "                    support_loss = F.cross_entropy(support_outputs, support_labels)\n",
        "                    inner_optimizer.step(support_loss)\n",
        "\n",
        "                # Query loss calculation (meta-objective)\n",
        "                query_outputs = fast_model(query_images)\n",
        "                query_loss = F.cross_entropy(query_outputs, query_labels)\n",
        "\n",
        "                # Calculate accuracy for monitoring\n",
        "                _, predicted = torch.max(query_outputs.data, 1)\n",
        "                correct += (predicted == query_labels).sum().item()\n",
        "                total += query_labels.size(0)\n",
        "\n",
        "                # Meta update\n",
        "                query_loss.backward()\n",
        "\n",
        "            # Apply meta-update to the original model\n",
        "            meta_optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            total_loss += query_loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{total_loss/(episode_idx+1):.4f}',\n",
        "                'acc': f'{100*correct/total:.2f}%'\n",
        "            })\n",
        "\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Lky0UaZh75NW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Configuration\n",
        "num_ways = 5  # Number of classes per episode\n",
        "num_shots = 5  # Number of support samples per class for training\n",
        "num_epochs = 50  # Set to 50 as requested\n",
        "num_episodes_per_epoch = 100\n",
        "num_shots_test = [1, 5, 10]  # Different shot settings for testing\n",
        "\n",
        "# Setup data directory\n",
        "data_dir = \"/home/ifihan/multi-dease-detection/data_set\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "model = MAMLResNet(num_ways=num_ways, pretrained=True).to(device)\n",
        "\n",
        "# Configure meta-optimizer with weight decay for regularization\n",
        "meta_optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = FewShotDataset(\n",
        "    data_dir=data_dir,\n",
        "    split=\"train\",\n",
        "    num_ways=num_ways,\n",
        "    num_support=num_shots,\n",
        "    num_query=10,\n",
        "    num_episodes=num_episodes_per_epoch,\n",
        "    augment=True  # Use data augmentation for training\n",
        ")\n",
        "train_loader = get_data_loader(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': []\n",
        "}\n",
        "\n",
        "# Best model tracking\n",
        "best_train_acc = 0.0\n",
        "# best_model_path = f'maml_resnet50_{num_ways}way_{num_shots}shot_best.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    # Train for one epoch\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_maml_epoch(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        meta_optimizer=meta_optimizer,\n",
        "        epoch=epoch,\n",
        "        num_inner_steps=5,\n",
        "        inner_lr=0.01,\n",
        "        device=device\n",
        "    )\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # Save training metrics\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "\n",
        "    print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    # Save best model based on training accuracy (no validation set)\n",
        "    if train_acc > best_train_acc:\n",
        "        best_train_acc = train_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': meta_optimizer.state_dict(),\n",
        "            'train_accuracy': train_acc,\n",
        "        }, best_model_path)\n",
        "        print(f\"Saved new best model with training accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "    # Save checkpoint every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        checkpoint_path = f'maml_resnet50_{num_ways}way_{num_shots}shot_epoch{epoch}.pth'\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': meta_optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'history': history,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Saved checkpoint at epoch {epoch}\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs + 1), history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs + 1), history['train_acc'])\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('maml_training_curves.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_path = \"/home/ifihan/multi-dease-detection/maml_resnet50_5way_5shot_best.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_maml(\n",
        "    model,\n",
        "    test_loader,\n",
        "    num_inner_steps=10,\n",
        "    inner_lr=0.01,\n",
        "    device=\"cuda\",\n",
        "    shots=[1, 5, 10]\n",
        "):\n",
        "    \"\"\"\n",
        "    Test the MAML model with different shot settings.\n",
        "    Fixes the class count issue and gradient calculation issue.\n",
        "    \"\"\"\n",
        "    # Dictionary to store results for each shot setting\n",
        "    results = {}\n",
        "    \n",
        "    # Check the number of classes in the test directory\n",
        "    test_dir = os.path.join(data_dir, \"test\")\n",
        "    available_classes = [c for c in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, c))]\n",
        "    actual_num_ways = min(num_ways, len(available_classes))\n",
        "    \n",
        "    if actual_num_ways < num_ways:\n",
        "        print(f\"Warning: Found only {len(available_classes)} classes in test set, but num_ways={num_ways}.\")\n",
        "        print(f\"Will use num_ways={actual_num_ways} for testing.\")\n",
        "\n",
        "    for shot in shots:\n",
        "        print(f\"\\nEvaluating {shot}-shot performance...\")\n",
        "\n",
        "        # Create test dataset for this shot setting\n",
        "        test_dataset = FewShotDataset(\n",
        "            data_dir=data_dir,\n",
        "            split=\"test\",\n",
        "            num_ways=actual_num_ways,  # Use adjusted num_ways\n",
        "            num_support=shot,\n",
        "            num_query=15,\n",
        "            num_episodes=100,\n",
        "            augment=False\n",
        "        )\n",
        "        \n",
        "        shot_test_loader = get_data_loader(test_dataset, shuffle=False)\n",
        "\n",
        "        # Evaluate with this loader\n",
        "        model.eval()\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        all_accuracies = []\n",
        "\n",
        "        for episode_idx, (support, query, episode_classes) in enumerate(tqdm(shot_test_loader, desc=f\"Testing {shot}-shot\")):\n",
        "            support_images, support_labels = support\n",
        "            query_images, query_labels = query\n",
        "\n",
        "            support_images = support_images.squeeze(0).to(device)\n",
        "            support_labels = support_labels.squeeze(0).to(device)\n",
        "            query_images = query_images.squeeze(0).to(device)\n",
        "            query_labels = query_labels.squeeze(0).to(device)\n",
        "\n",
        "            # Create a copy of the model for this episode\n",
        "            episode_model = copy.deepcopy(model)\n",
        "            \n",
        "            # Make sure we enable gradient calculation for adaptation\n",
        "            episode_model.train()\n",
        "            for param in episode_model.parameters():\n",
        "                param.requires_grad = True\n",
        "                \n",
        "            episode_optimizer = torch.optim.SGD(\n",
        "                filter(lambda p: p.requires_grad, episode_model.parameters()),\n",
        "                lr=inner_lr\n",
        "            )\n",
        "\n",
        "            # Inner loop adaptation\n",
        "            for _ in range(num_inner_steps):\n",
        "                # Zero gradients first\n",
        "                episode_optimizer.zero_grad()\n",
        "                \n",
        "                # Forward pass\n",
        "                support_outputs = episode_model(support_images)\n",
        "                support_loss = F.cross_entropy(support_outputs, support_labels)\n",
        "                \n",
        "                # Backward pass\n",
        "                support_loss.backward()\n",
        "                \n",
        "                # Step optimizer\n",
        "                episode_optimizer.step()\n",
        "\n",
        "            # Evaluation on query set\n",
        "            episode_model.eval()\n",
        "            with torch.no_grad():\n",
        "                query_outputs = episode_model(query_images)\n",
        "                preds = query_outputs.argmax(dim=1)\n",
        "                episode_correct = (preds == query_labels).sum().item()\n",
        "                episode_accuracy = episode_correct / len(query_labels)\n",
        "                all_accuracies.append(episode_accuracy)\n",
        "\n",
        "                total_correct += episode_correct\n",
        "                total_samples += len(query_labels)\n",
        "\n",
        "        # Calculate overall accuracy and confidence interval\n",
        "        mean_accuracy = total_correct / total_samples\n",
        "        std_accuracy = np.std(all_accuracies)\n",
        "        ci95 = 1.96 * std_accuracy / np.sqrt(len(all_accuracies))\n",
        "\n",
        "        # Store results\n",
        "        results[shot] = {\n",
        "            'accuracy': mean_accuracy,\n",
        "            'ci95': ci95\n",
        "        }\n",
        "\n",
        "        print(f\"{shot}-shot Test Accuracy: {mean_accuracy*100:.2f}% ± {ci95*100:.2f}%\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading best model for final evaluation...\n",
            "Warning: Found only 4 classes in test set, but num_ways=5.\n",
            "Will use num_ways=4 for testing.\n",
            "\n",
            "Evaluating 1-shot performance...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing 1-shot:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing 1-shot: 100%|██████████| 100/100 [00:33<00:00,  2.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-shot Test Accuracy: 33.08% ± 1.46%\n",
            "\n",
            "Evaluating 5-shot performance...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing 5-shot: 100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5-shot Test Accuracy: 43.92% ± 1.26%\n",
            "\n",
            "Evaluating 10-shot performance...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing 10-shot: 100%|██████████| 100/100 [01:43<00:00,  1.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-shot Test Accuracy: 51.17% ± 1.41%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Fix just the test_maml function call\n",
        "best_model_path = \"/home/ifihan/multi-dease-detection/maml_resnet50_5way_5shot_best.pth\"\n",
        "\n",
        "# Load the best model\n",
        "print(\"\\nLoading best model for final evaluation...\")\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Test with different shot settings\n",
        "test_results = test_maml(\n",
        "    model=model,\n",
        "    test_loader=None,  # Not used as we create loaders inside the function\n",
        "    num_inner_steps=10,\n",
        "    inner_lr=0.01,\n",
        "    device=device,\n",
        "    shots=num_shots_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FC1zPK9NamYN",
        "ZJ1-_8tG8YBt",
        "dDYFGE-JsMuT"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
