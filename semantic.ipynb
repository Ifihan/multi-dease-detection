{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UcIJj79qpwG"
      },
      "source": [
        "# Few Shot Sampling of Blood Smear Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC1zPK9NamYN"
      },
      "source": [
        "## Custom Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75s1YnBzFJWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fByqZWp5vcNG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def resize_and_pad(image, target_size=(224, 224), fill=0):\n",
        "    \"\"\"\n",
        "    Resize an image to fit within the target size while preserving the aspect ratio,\n",
        "    then pad the shorter sides with a constant value to reach the target size.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): Input image.\n",
        "        target_size (tuple): Target size (width, height), default (224, 224).\n",
        "        fill (int or tuple): Padding fill value (e.g., 0 for black, 255 for white).\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: Resized and padded image of size target_size.\n",
        "    \"\"\"\n",
        "    # Get original dimensions\n",
        "    width, height = image.size\n",
        "    target_width, target_height = target_size\n",
        "\n",
        "    # Calculate scaling factor to fit within target size\n",
        "    scale = min(target_width / width, target_height / height)\n",
        "    new_width = int(width * scale)\n",
        "    new_height = int(height * scale)\n",
        "\n",
        "    # Resize image with correct (height, width) order\n",
        "    resized_image = transforms.functional.resize(image, (new_height, new_width))\n",
        "\n",
        "    # Calculate padding to center the image\n",
        "    padding_left = (target_width - new_width) // 2\n",
        "    padding_top = (target_height - new_height) // 2\n",
        "    # Ensure padding adds up exactly to the difference\n",
        "    padding_right = target_width - new_width - padding_left\n",
        "    padding_bottom = target_height - new_height - padding_top\n",
        "\n",
        "    # Apply padding\n",
        "    padded_image = transforms.functional.pad(\n",
        "        resized_image,\n",
        "        padding=(padding_left, padding_top, padding_right, padding_bottom),\n",
        "        fill=fill\n",
        "    )\n",
        "\n",
        "    return padded_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osiwpi1mvvjG"
      },
      "outputs": [],
      "source": [
        "# Basic Image transformation\n",
        "def get_base_transforms(target_size=(320, 320), use_grayscale=False):\n",
        "    \"\"\"\n",
        "    Returns a composed set of basic image transformations for preprocessing input images.\n",
        "\n",
        "    Parameters:\n",
        "    - target_size (tuple): The desired output size (height, width) of the image after resizing and padding.\n",
        "    - use_grayscale (bool): If True, converts the image to grayscale with 3 channels before applying other transformations.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Optional grayscale conversion with 3 output channels,\n",
        "        - Resizing and padding the image to match the target size,\n",
        "        - Conversion to tensor,\n",
        "        - Normalization using ImageNet mean and standard deviation.\n",
        "    \"\"\"\n",
        "    base_transforms = [\n",
        "        transforms.Lambda(lambda img: resize_and_pad(img, target_size=target_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        "\n",
        "    if use_grayscale:\n",
        "        base_transforms.insert(0, transforms.Grayscale(num_output_channels=3))  # Keep 3 channels for compatibility\n",
        "    return transforms.Compose(base_transforms)\n",
        "\n",
        "\n",
        "# Data augmentation transforms\n",
        "def get_augmentation_transforms():\n",
        "    \"\"\"\n",
        "    Returns a composed set of data augmentation transformations to artificially expand the training dataset.\n",
        "\n",
        "    This function applies a series of random transformations to simulate variations in brightness, contrast, orientation,\n",
        "    and color mode, helping the model generalize better.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Random brightness and contrast adjustment (ColorJitter),\n",
        "        - Random horizontal and vertical flipping,\n",
        "        - Random rotation by up to Â±10 degrees,\n",
        "        - Random conversion to grayscale with a 20% probability.\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Simulate lighting/stain variations\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomGrayscale(p=0.2)  # Optional: Randomly apply grayscale as part of augmentation\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQtpp1OmFV25"
      },
      "outputs": [],
      "source": [
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, num_ways=5, num_support=5,\n",
        "                 num_query=10, num_episodes=100, target_size=(224, 224),\n",
        "                 use_grayscale=False,\n",
        "                 augment=False,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Path to dataset directory\n",
        "            split (str): One of 'train', 'validation', or 'test'\n",
        "            num_ways (int): Number of classes per episode\n",
        "            num_support (int): Number of support samples per class (i.e. number of shots)\n",
        "            num_query (int): Number of query samples per class\n",
        "            num_episodes (int): Number of episodes per epoch\n",
        "            use_grayscale(bool),  Use grayscale or not\n",
        "            augment(bool),        For data augmentation technique\n",
        "        \"\"\"\n",
        "        self.split_dir = os.path.join(data_dir, split)\n",
        "        self.num_ways = num_ways\n",
        "        self.num_support = num_support\n",
        "        self.num_query = num_query\n",
        "        self.num_episodes = num_episodes\n",
        "        base_transform = get_base_transforms(target_size, use_grayscale)\n",
        "        if augment:\n",
        "            augmentation_transform = get_augmentation_transforms()\n",
        "            self.transform = transforms.Compose([augmentation_transform, base_transform])\n",
        "        else:\n",
        "            self.transform = base_transform\n",
        "\n",
        "        # Load class directories and their images\n",
        "        self.classes = [c for c in os.listdir(self.split_dir)\n",
        "                       if os.path.isdir(os.path.join(self.split_dir, c))]\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}  # Map class names to indices\n",
        "        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}  # Map indices to class names\n",
        "        self.class_images = {\n",
        "            c: [os.path.join(self.split_dir, c, img)\n",
        "                for img in os.listdir(os.path.join(self.split_dir, c))]\n",
        "            for c in self.classes\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_episodes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Randomly select N classes for this episode\n",
        "        selected_classes = np.random.choice(self.classes, self.num_ways, replace=False)\n",
        "\n",
        "        support_images = []\n",
        "        support_labels = []\n",
        "        query_images = []\n",
        "        query_labels = []\n",
        "\n",
        "        for label_idx, class_name in enumerate(selected_classes):\n",
        "            all_images = self.class_images[class_name]\n",
        "            if len(all_images) < self.num_support + self.num_query:\n",
        "                raise ValueError(\n",
        "                    f\"Class {class_name} has only {len(all_images)} images. \"\n",
        "                    f\"Need at least {self.num_support + self.num_query}.\"\n",
        "                )\n",
        "\n",
        "            # Randomly select support and query images\n",
        "            selected_indices = np.random.choice(\n",
        "                len(all_images),\n",
        "                self.num_support + self.num_query,\n",
        "                replace=False #True#########################################################################\n",
        "            )\n",
        "            support_paths = [all_images[i] for i in selected_indices[:self.num_support]]\n",
        "            query_paths = [all_images[i] for i in selected_indices[self.num_support:]]\n",
        "\n",
        "            # Load and transform support images\n",
        "            for path in support_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                support_images.append(img)\n",
        "                support_labels.append(label_idx)\n",
        "\n",
        "            # Load and transform query images\n",
        "            for path in query_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                query_images.append(img)\n",
        "                query_labels.append(label_idx)\n",
        "\n",
        "        # Shuffle the support and query sets\n",
        "        support_indices = np.arange(len(support_images))\n",
        "        query_indices = np.arange(len(query_images))\n",
        "        np.random.shuffle(support_indices)\n",
        "        np.random.shuffle(query_indices)\n",
        "\n",
        "        support_images = [support_images[i] for i in support_indices]\n",
        "        support_labels = [support_labels[i] for i in support_indices]\n",
        "        query_images = [query_images[i] for i in query_indices]\n",
        "        query_labels = [query_labels[i] for i in query_indices]\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        support_set = (\n",
        "            torch.stack(support_images),\n",
        "            torch.tensor(support_labels, dtype=torch.long)\n",
        "        )\n",
        "        query_set = (\n",
        "            torch.stack(query_images),\n",
        "            torch.tensor(query_labels, dtype=torch.long)\n",
        "        )\n",
        "        # Store the selected class names for this episode\n",
        "        selected_classes = [str(cls) for cls in selected_classes]\n",
        "        episode_classes = selected_classes\n",
        "\n",
        "        return support_set, query_set, episode_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paGubE7bFSLp"
      },
      "outputs": [],
      "source": [
        "def get_data_loader(dataset, batch_size=1, shuffle=True):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the dataset.\n",
        "    Note: Batch size should typically be 1 for few-shot learning,\n",
        "    as each episode is a separate task.\n",
        "    \"\"\"\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJNGAZFtcFtR"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2bzS2kCsX30"
      },
      "source": [
        "### Semantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nljwAgVFLyu",
        "outputId": "a8494849-98ae-4591-ff3c-dc85273ef2d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.optim as optim\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- BioBERT for Disease Embeddings ---\n",
        "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
        "bert_model = BertModel.from_pretrained('dmis-lab/biobert-v1.1').to(device)\n",
        "\n",
        "def get_disease_embedding(disease_name):\n",
        "    \"\"\"Get BioBERT embedding for a disease name.\"\"\"\n",
        "    inputs = tokenizer(disease_name, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze()  # [768]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBDLYv_V8V2g"
      },
      "outputs": [],
      "source": [
        "# --- Clinical Features ---\n",
        "# One-hot encoding for cell shape: [round, elongated, irregular, other]\n",
        "clinical_features = {\n",
        "    'iron_deficiency_anemia': torch.tensor([0, 1, 0, 0], dtype=torch.float),  # Elongated\n",
        "    'rouleaux_formation': torch.tensor([0, 0, 0, 1], dtype=torch.float),      # Other (stacked)\n",
        "    'essential_thrombocytopemia': torch.tensor([0, 0, 0, 1], dtype=torch.float),  # Other\n",
        "    'hypersegmented_neutrophils': torch.tensor([0, 0, 1, 0], dtype=torch.float),      # Irregular\n",
        "    'myelodysplastic_syndrome_buyomes': torch.tensor([0, 0, 1, 0], dtype=torch.float), # Irregular\n",
        "    'platelet_clumps': torch.tensor([0, 0, 0, 1], dtype=torch.float),         # Other\n",
        "    'myeloid_leukemia': torch.tensor([0, 0, 1, 0], dtype=torch.float),        # Irregular\n",
        "    'filariasis': torch.tensor([0, 0, 0, 1], dtype=torch.float),              # Other\n",
        "    'howell_jolly_bodies': torch.tensor([0, 0, 1, 0], dtype=torch.float),     # Irregular\n",
        "    'chronic_lymphocytic_leukemia': torch.tensor([0, 0, 1, 0], dtype=torch.float), # Irregular\n",
        "    'neutropenia': torch.tensor([0, 0, 0, 1], dtype=torch.float),             # Other\n",
        "    'schistocytes': torch.tensor([0, 0, 1, 0], dtype=torch.float),            # Irregular\n",
        "    'immune_thrombocytopenic_purpura': torch.tensor([0, 0, 0, 1], dtype=torch.float), # Other\n",
        "    'basophilic_stippling': torch.tensor([0, 0, 1, 0], dtype=torch.float),    # Irregular\n",
        "    'hemophagocytic_lymphohistiocytosis': torch.tensor([0, 0, 0, 1], dtype=torch.float), # Other\n",
        "    'target_cells': torch.tensor([1, 0, 0, 0], dtype=torch.float),            # Round\n",
        "    'spherocytosis': torch.tensor([1, 0, 0, 0], dtype=torch.float),           # Round\n",
        "    'reticulocytosis': torch.tensor([0, 0, 0, 1], dtype=torch.float),         # Other\n",
        "    'babesiosis': torch.tensor([0, 0, 0, 1], dtype=torch.float),              # Other\n",
        "    'myelofibrosis': torch.tensor([0, 0, 0, 1], dtype=torch.float),           # Other\n",
        "    'giant_platelets': torch.tensor([0, 0, 0, 1], dtype=torch.float),         # Other\n",
        "    'chronic_myelogenous_leukemia': torch.tensor([0, 0, 1, 0], dtype=torch.float), # Irregular\n",
        "    'basophilic': torch.tensor([0, 0, 0, 1], dtype=torch.float),               # Other (staining property)\n",
        "    'ehrlichiosis': torch.tensor([0, 0, 0, 1], dtype=torch.float),            # Other\n",
        "    'elliptocytes_ovalocytes': torch.tensor([0, 1, 0, 0], dtype=torch.float), # Elongated\n",
        "    'atypical_lymphocytes': torch.tensor([0, 0, 1, 0], dtype=torch.float), # Irregular\n",
        "    'hemolytic_uremic_syndrome': torch.tensor([0, 0, 1, 0], dtype=torch.float), # Irregular\n",
        "    'anaplasmosis': torch.tensor([0, 0, 0, 1], dtype=torch.float),            # Other\n",
        "    'malaria': torch.tensor([0, 0, 0, 1], dtype=torch.float),                 # Other\n",
        "    'sickle_cell': torch.tensor([0, 1, 0, 0], dtype=torch.float),      # Elongated\n",
        "    'trypanosomiasis': torch.tensor([0, 0, 0, 1], dtype=torch.float),         # Other\n",
        "    'leukemia': torch.tensor([0, 0, 1, 0], dtype=torch.float)                 # Irregular\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibbo2cEU-Y50"
      },
      "outputs": [],
      "source": [
        "# --- Dataset ---\n",
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, num_ways=5, num_support=5,\n",
        "                 num_query=10, num_episodes=100, target_size=(224, 224), augment=False):\n",
        "        self.split_dir = os.path.join(data_dir, split)\n",
        "        self.num_ways = num_ways\n",
        "        self.num_support = num_support\n",
        "        self.num_query = num_query\n",
        "        self.num_episodes = num_episodes\n",
        "        base_transform = transforms.Compose([\n",
        "            transforms.Resize(target_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        if augment:\n",
        "            augmentation_transform = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(10),\n",
        "            ])\n",
        "            self.transform = transforms.Compose([augmentation_transform, base_transform])\n",
        "        else:\n",
        "            self.transform = base_transform\n",
        "\n",
        "        self.classes = [c for c in os.listdir(self.split_dir)\n",
        "                        if os.path.isdir(os.path.join(self.split_dir, c))]\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        self.class_images = {\n",
        "            c: [os.path.join(self.split_dir, c, img)\n",
        "                for img in os.listdir(os.path.join(self.split_dir, c))]\n",
        "            for c in self.classes\n",
        "        }\n",
        "        # Semantic embeddings and clinical features\n",
        "        self.class_embeddings = {cls: get_disease_embedding(cls) for cls in self.classes}\n",
        "\n",
        "        # Find and print classes not in clinical_features\n",
        "        missing_classes = [cls for cls in self.classes if cls not in clinical_features]\n",
        "        if missing_classes:\n",
        "            print(\"Classes not found in clinical_features:\", missing_classes)\n",
        "\n",
        "        self.class_clinical = {cls: clinical_features.get(cls, torch.zeros(4, dtype=torch.float)) for cls in self.classes}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_episodes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        selected_classes = np.random.choice(self.classes, self.num_ways, replace=False)\n",
        "        support_images, support_labels = [], []\n",
        "        query_images, query_labels = [], []\n",
        "        selected_embeddings = []\n",
        "        selected_clinical = []\n",
        "\n",
        "        for label_idx, class_name in enumerate(selected_classes):\n",
        "            all_images = self.class_images[class_name]\n",
        "            if len(all_images) < self.num_support + self.num_query:\n",
        "                raise ValueError(f\"Class {class_name} has insufficient images.\")\n",
        "            selected_indices = np.random.choice(len(all_images), self.num_support + self.num_query, replace=False)\n",
        "            support_paths = [all_images[i] for i in selected_indices[:self.num_support]]\n",
        "            query_paths = [all_images[i] for i in selected_indices[self.num_support:]]\n",
        "\n",
        "            for path in support_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                support_images.append(img)\n",
        "                support_labels.append(label_idx)\n",
        "\n",
        "            for path in query_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                query_images.append(img)\n",
        "                query_labels.append(label_idx)\n",
        "\n",
        "            selected_embeddings.append(self.class_embeddings[class_name])\n",
        "            selected_clinical.append(self.class_clinical[class_name])\n",
        "\n",
        "        support_indices = np.random.permutation(len(support_images))\n",
        "        query_indices = np.random.permutation(len(query_images))\n",
        "        support_images = [support_images[i] for i in support_indices]\n",
        "        support_labels = [support_labels[i] for i in support_indices]\n",
        "        query_images = [query_images[i] for i in query_indices]\n",
        "        query_labels = [query_labels[i] for i in query_indices]\n",
        "\n",
        "        support_set = (torch.stack(support_images), torch.tensor(support_labels, dtype=torch.long))\n",
        "        query_set = (torch.stack(query_images), torch.tensor(query_labels, dtype=torch.long))\n",
        "        embeddings = torch.stack(selected_embeddings)  # [num_ways, 768]\n",
        "        clinical = torch.stack(selected_clinical)      # [num_ways, clinical_dim]\n",
        "        return support_set, query_set, embeddings, clinical\n",
        "\n",
        "def get_data_loader(dataset, batch_size=1, shuffle=True):\n",
        "    return DataLoader(dataset, batch_size=batch_size,\n",
        "                      shuffle=shuffle,\n",
        "                      num_workers=0,\n",
        "                      pin_memory=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxYDymuN-cCa"
      },
      "outputs": [],
      "source": [
        "# --- Model ---\n",
        "class SemanticPrototypicalNetwork(nn.Module):\n",
        "    def __init__(self, backbone, semantic_dim=768,\n",
        "                 clinical_dim=4, fusion_dim=2048,\n",
        "                 dropout_p=0.2):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()  # Remove original classifier\n",
        "        self.semantic_proj = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(semantic_dim, fusion_dim)\n",
        "        )\n",
        "        self.clinical_proj = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(clinical_dim, fusion_dim)\n",
        "        )\n",
        "        self.visual_proj = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(2048, fusion_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, support_images, support_labels, query_images, class_embeddings, class_clinical):\n",
        "        # Extract visual features\n",
        "        support_features = self.backbone(support_images)  # [N_s, 2048]\n",
        "        query_features = self.backbone(query_images)      # [N_q, 2048]\n",
        "\n",
        "        # Project features\n",
        "        support_features_proj = self.visual_proj(support_features)  # [N_s, fusion_dim]\n",
        "        query_features_proj = self.visual_proj(query_features)      # [N_q, fusion_dim]\n",
        "        class_embeddings_proj = self.semantic_proj(class_embeddings)  # [num_ways, fusion_dim]\n",
        "        class_clinical_proj = self.clinical_proj(class_clinical)      # [num_ways, fusion_dim]\n",
        "\n",
        "        # Compute visual prototypes\n",
        "        unique_labels = torch.unique(support_labels)\n",
        "        visual_prototypes = []\n",
        "        for label in unique_labels:\n",
        "            mask = support_labels == label\n",
        "            proto = support_features_proj[mask].mean(dim=0)\n",
        "            visual_prototypes.append(proto)\n",
        "        visual_prototypes = torch.stack(visual_prototypes)  # [num_ways, fusion_dim]\n",
        "\n",
        "        # Fuse with semantic and clinical features\n",
        "        fused_prototypes = visual_prototypes + class_embeddings_proj + class_clinical_proj  # Simple addition\n",
        "\n",
        "        # Compute distances\n",
        "        distances = torch.cdist(query_features_proj, fused_prototypes)  # [N_q, num_ways]\n",
        "        return -distances  # Logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7KyEiXD-eZC"
      },
      "outputs": [],
      "source": [
        "# --- Training and Evaluation ---\n",
        "data_dir = \"/content/drive/MyDrive/Computer vision with few shot sampling focus group/data_set\"  # Update this path\n",
        "\n",
        "# Training dataset\n",
        "train_dataset = FewShotDataset(\n",
        "    data_dir=data_dir,\n",
        "    split=\"train\",\n",
        "    num_ways=5,\n",
        "    num_support=5,\n",
        "    num_query=10,\n",
        "    num_episodes=100,\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "train_loader = get_data_loader(train_dataset)\n",
        "\n",
        "# Initialize model\n",
        "backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "model = SemanticPrototypicalNetwork(backbone, dropout_p=0.2).to(device)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "# Freeze the backbone\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Ensure projection layers are trainable\n",
        "for param in model.visual_proj.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.semantic_proj.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.clinical_proj.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVSZFmLDGzaE",
        "outputId": "1f270b67-72d2-4899-aea3-b8b55aabfe8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 1.7923, Acc: 0.5506\n",
            "Saved best model with accuracy 0.5506 to /content/drive/MyDrive/Computer vision with few shot sampling focus group/best_semantic_model.pth\n",
            "Epoch 2/5, Loss: 0.8828, Acc: 0.7440\n",
            "Saved best model with accuracy 0.7440 to /content/drive/MyDrive/Computer vision with few shot sampling focus group/best_semantic_model.pth\n",
            "Epoch 3/5, Loss: 0.6345, Acc: 0.8056\n",
            "Saved best model with accuracy 0.8056 to /content/drive/MyDrive/Computer vision with few shot sampling focus group/best_semantic_model.pth\n",
            "Epoch 4/5, Loss: 0.5610, Acc: 0.8320\n",
            "Saved best model with accuracy 0.8320 to /content/drive/MyDrive/Computer vision with few shot sampling focus group/best_semantic_model.pth\n",
            "Epoch 5/5, Loss: 0.4871, Acc: 0.8472\n",
            "Saved best model with accuracy 0.8472 to /content/drive/MyDrive/Computer vision with few shot sampling focus group/best_semantic_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_acc = 0.0  # Initialize best accuracy\n",
        "best_model_path = \"/content/drive/MyDrive/Computer vision with few shot sampling focus group/best_semantic_model.pth\"\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    for support_set, query_set, embeddings, clinical in train_loader:\n",
        "        support_images, support_labels = support_set\n",
        "        query_images, query_labels = query_set\n",
        "        embeddings = embeddings.to(device)\n",
        "        clinical = clinical.to(device)\n",
        "\n",
        "        support_images = support_images.to(device)\n",
        "        support_labels = support_labels.to(device)\n",
        "        query_images = query_images.to(device)\n",
        "        query_labels = query_labels.to(device)\n",
        "\n",
        "        if support_images.dim() == 5:\n",
        "            support_images = support_images.squeeze(0)\n",
        "        if query_images.dim() == 5:\n",
        "            query_images = query_images.squeeze(0)\n",
        "        if support_labels.dim() == 2:\n",
        "            support_labels = support_labels.squeeze(0)\n",
        "        if query_labels.dim() == 2:\n",
        "            query_labels = query_labels.squeeze(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(support_images, support_labels, query_images, embeddings, clinical)\n",
        "        logits = logits.view(-1, logits.shape[-1])  # Reshape logits\n",
        "        loss = criterion(logits, query_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == query_labels).float().mean()\n",
        "        train_acc += acc.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc /= len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    scheduler.step(train_loss)\n",
        "\n",
        "    #Save best model\n",
        "    if train_acc > best_acc:\n",
        "        best_acc = train_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Saved best model with accuracy {best_acc:.4f} to {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGeAkZzEQz8z",
        "outputId": "501f81da-9408-4638-fac9-b73aa3ccd970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-Shot Test Accuracy: 0.3730\n",
            "5-Shot Test Accuracy: 0.4853\n",
            "10-Shot Test Accuracy: 0.5128\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model (adjust the path to your saved model)\n",
        "#model.load_state_dict(torch.load(\"/content/drive/MyDrive/Computer vision with few shot sampling focus group/best_semantic_model.pth\"))  # Ensure this file exists\n",
        "\n",
        "# Evaluate 1-shot, 5-shot, 10-shot\n",
        "test_shots = [1, 5, 10]\n",
        "for num_shots in test_shots:\n",
        "    test_dataset = FewShotDataset(\n",
        "        data_dir=data_dir,\n",
        "        split=\"test\",\n",
        "        num_ways=4,  # 4 test diseases\n",
        "        num_support=num_shots,\n",
        "        num_query=10,\n",
        "        num_episodes=100,\n",
        "        augment=False\n",
        "    )\n",
        "    test_loader = get_data_loader(test_dataset)\n",
        "\n",
        "    model.eval()\n",
        "    test_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for support_set, query_set, embeddings, clinical in test_loader:\n",
        "            support_images, support_labels = support_set\n",
        "            query_images, query_labels = query_set\n",
        "            embeddings = embeddings.to(device)\n",
        "            clinical = clinical.to(device)\n",
        "\n",
        "            support_images = support_images.to(device)\n",
        "            support_labels = support_labels.to(device)\n",
        "            query_images = query_images.to(device)\n",
        "            query_labels = query_labels.to(device)\n",
        "\n",
        "            # Remove extra batch dimension if present\n",
        "            if support_images.dim() == 5:\n",
        "                support_images = support_images.squeeze(0)\n",
        "            if query_images.dim() == 5:\n",
        "                query_images = query_images.squeeze(0)\n",
        "            if support_labels.dim() == 2:\n",
        "                support_labels = support_labels.squeeze(0)\n",
        "            if query_labels.dim() == 2:\n",
        "                query_labels = query_labels.squeeze(0)\n",
        "\n",
        "            logits = model(support_images, support_labels, query_images, embeddings, clinical)\n",
        "            logits = logits.view(-1, logits.shape[-1])  # Reshape logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            acc = (preds == query_labels).float().mean()\n",
        "            test_acc += acc.item()\n",
        "\n",
        "    test_acc /= len(test_loader)\n",
        "    print(f\"{num_shots}-Shot Test Accuracy: {test_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FC1zPK9NamYN",
        "ZJ1-_8tG8YBt",
        "dDYFGE-JsMuT"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
