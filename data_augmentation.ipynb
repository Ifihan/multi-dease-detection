{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FC1zPK9NamYN",
        "ZJ1-_8tG8YBt",
        "dDYFGE-JsMuT"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Sampling of Blood Smear Images"
      ],
      "metadata": {
        "id": "4UcIJj79qpwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataloader"
      ],
      "metadata": {
        "id": "FC1zPK9NamYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75s1YnBzFJWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def resize_and_pad(image, target_size=(224, 224), fill=0):\n",
        "    \"\"\"\n",
        "    Resize an image to fit within the target size while preserving the aspect ratio,\n",
        "    then pad the shorter sides with a constant value to reach the target size.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): Input image.\n",
        "        target_size (tuple): Target size (width, height), default (224, 224).\n",
        "        fill (int or tuple): Padding fill value (e.g., 0 for black, 255 for white).\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: Resized and padded image of size target_size.\n",
        "    \"\"\"\n",
        "    # Get original dimensions\n",
        "    width, height = image.size\n",
        "    target_width, target_height = target_size\n",
        "\n",
        "    # Calculate scaling factor to fit within target size\n",
        "    scale = min(target_width / width, target_height / height)\n",
        "    new_width = int(width * scale)\n",
        "    new_height = int(height * scale)\n",
        "\n",
        "    # Resize image with correct (height, width) order\n",
        "    resized_image = transforms.functional.resize(image, (new_height, new_width))\n",
        "\n",
        "    # Calculate padding to center the image\n",
        "    padding_left = (target_width - new_width) // 2\n",
        "    padding_top = (target_height - new_height) // 2\n",
        "    # Ensure padding adds up exactly to the difference\n",
        "    padding_right = target_width - new_width - padding_left\n",
        "    padding_bottom = target_height - new_height - padding_top\n",
        "\n",
        "    # Apply padding\n",
        "    padded_image = transforms.functional.pad(\n",
        "        resized_image,\n",
        "        padding=(padding_left, padding_top, padding_right, padding_bottom),\n",
        "        fill=fill\n",
        "    )\n",
        "\n",
        "    return padded_image"
      ],
      "metadata": {
        "id": "fByqZWp5vcNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Image transformation\n",
        "def get_base_transforms(target_size=(320, 320), use_grayscale=False):\n",
        "    \"\"\"\n",
        "    Returns a composed set of basic image transformations for preprocessing input images.\n",
        "\n",
        "    Parameters:\n",
        "    - target_size (tuple): The desired output size (height, width) of the image after resizing and padding.\n",
        "    - use_grayscale (bool): If True, converts the image to grayscale with 3 channels before applying other transformations.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Optional grayscale conversion with 3 output channels,\n",
        "        - Resizing and padding the image to match the target size,\n",
        "        - Conversion to tensor,\n",
        "        - Normalization using ImageNet mean and standard deviation.\n",
        "    \"\"\"\n",
        "    base_transforms = [\n",
        "        transforms.Lambda(lambda img: resize_and_pad(img, target_size=target_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        "\n",
        "    if use_grayscale:\n",
        "        base_transforms.insert(0, transforms.Grayscale(num_output_channels=3))  # Keep 3 channels for compatibility\n",
        "    return transforms.Compose(base_transforms)\n",
        "\n",
        "\n",
        "# Data augmentation transforms\n",
        "def get_augmentation_transforms():\n",
        "    \"\"\"\n",
        "    Returns a composed set of data augmentation transformations to artificially expand the training dataset.\n",
        "\n",
        "    This function applies a series of random transformations to simulate variations in brightness, contrast, orientation,\n",
        "    and color mode, helping the model generalize better.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Random brightness and contrast adjustment (ColorJitter),\n",
        "        - Random horizontal and vertical flipping,\n",
        "        - Random rotation by up to Â±10 degrees,\n",
        "        - Random conversion to grayscale with a 20% probability.\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Simulate lighting/stain variations\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomGrayscale(p=0.2)  # Optional: Randomly apply grayscale as part of augmentation\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "osiwpi1mvvjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, num_ways=5, num_support=5,\n",
        "                 num_query=10, num_episodes=100, target_size=(224, 224),\n",
        "                 use_grayscale=False,\n",
        "                 augment=False,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Path to dataset directory\n",
        "            split (str): One of 'train', 'validation', or 'test'\n",
        "            num_ways (int): Number of classes per episode\n",
        "            num_support (int): Number of support samples per class (i.e. number of shots)\n",
        "            num_query (int): Number of query samples per class\n",
        "            num_episodes (int): Number of episodes per epoch\n",
        "            use_grayscale(bool),  Use grayscale or not\n",
        "            augment(bool),        For data augmentation technique\n",
        "        \"\"\"\n",
        "        self.split_dir = os.path.join(data_dir, split)\n",
        "        self.num_ways = num_ways\n",
        "        self.num_support = num_support\n",
        "        self.num_query = num_query\n",
        "        self.num_episodes = num_episodes\n",
        "        base_transform = get_base_transforms(target_size, use_grayscale)\n",
        "        if augment:\n",
        "            augmentation_transform = get_augmentation_transforms()\n",
        "            self.transform = transforms.Compose([augmentation_transform, base_transform])\n",
        "        else:\n",
        "            self.transform = base_transform\n",
        "\n",
        "        # Load class directories and their images\n",
        "        self.classes = [c for c in os.listdir(self.split_dir)\n",
        "                       if os.path.isdir(os.path.join(self.split_dir, c))]\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}  # Map class names to indices\n",
        "        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}  # Map indices to class names\n",
        "        self.class_images = {\n",
        "            c: [os.path.join(self.split_dir, c, img)\n",
        "                for img in os.listdir(os.path.join(self.split_dir, c))]\n",
        "            for c in self.classes\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_episodes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Randomly select N classes for this episode\n",
        "        selected_classes = np.random.choice(self.classes, self.num_ways, replace=False)\n",
        "\n",
        "        support_images = []\n",
        "        support_labels = []\n",
        "        query_images = []\n",
        "        query_labels = []\n",
        "\n",
        "        for label_idx, class_name in enumerate(selected_classes):\n",
        "            all_images = self.class_images[class_name]\n",
        "            if len(all_images) < self.num_support + self.num_query:\n",
        "                raise ValueError(\n",
        "                    f\"Class {class_name} has only {len(all_images)} images. \"\n",
        "                    f\"Need at least {self.num_support + self.num_query}.\"\n",
        "                )\n",
        "\n",
        "            # Randomly select support and query images\n",
        "            selected_indices = np.random.choice(\n",
        "                len(all_images),\n",
        "                self.num_support + self.num_query,\n",
        "                replace=False #True#########################################################################\n",
        "            )\n",
        "            support_paths = [all_images[i] for i in selected_indices[:self.num_support]]\n",
        "            query_paths = [all_images[i] for i in selected_indices[self.num_support:]]\n",
        "\n",
        "            # Load and transform support images\n",
        "            for path in support_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                support_images.append(img)\n",
        "                support_labels.append(label_idx)\n",
        "\n",
        "            # Load and transform query images\n",
        "            for path in query_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                query_images.append(img)\n",
        "                query_labels.append(label_idx)\n",
        "\n",
        "        # Shuffle the support and query sets\n",
        "        support_indices = np.arange(len(support_images))\n",
        "        query_indices = np.arange(len(query_images))\n",
        "        np.random.shuffle(support_indices)\n",
        "        np.random.shuffle(query_indices)\n",
        "\n",
        "        support_images = [support_images[i] for i in support_indices]\n",
        "        support_labels = [support_labels[i] for i in support_indices]\n",
        "        query_images = [query_images[i] for i in query_indices]\n",
        "        query_labels = [query_labels[i] for i in query_indices]\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        support_set = (\n",
        "            torch.stack(support_images),\n",
        "            torch.tensor(support_labels, dtype=torch.long)\n",
        "        )\n",
        "        query_set = (\n",
        "            torch.stack(query_images),\n",
        "            torch.tensor(query_labels, dtype=torch.long)\n",
        "        )\n",
        "        # Store the selected class names for this episode\n",
        "        selected_classes = [str(cls) for cls in selected_classes]\n",
        "        episode_classes = selected_classes\n",
        "\n",
        "        return support_set, query_set, episode_classes"
      ],
      "metadata": {
        "id": "YQtpp1OmFV25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loader(dataset, batch_size=1, shuffle=True):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the dataset.\n",
        "    Note: Batch size should typically be 1 for few-shot learning,\n",
        "    as each episode is a separate task.\n",
        "    \"\"\"\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )"
      ],
      "metadata": {
        "id": "paGubE7bFSLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "tJNGAZFtcFtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentation Based"
      ],
      "metadata": {
        "id": "07Lht4lJsbX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GASr7PER9sWl",
        "outputId": "9c03d7c2-1dd5-4703-eb78-e775410a283d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_ways = 5\n",
        "num_shots_eval = [1, 5, 10]\n",
        "data_dir = '/content/drive/MyDrive/Computer vision with few shot sampling focus group/data_set'"
      ],
      "metadata": {
        "id": "XivOOejr7d1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SimCLR and CutMix"
      ],
      "metadata": {
        "id": "ZJ1-_8tG8YBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "class SimCLRDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for SimCLR pretraining.\n",
        "\n",
        "    This dataset loads images from a specified directory structure and returns two independently augmented\n",
        "    views of the same image, as required by SimCLR for contrastive learning.\n",
        "\n",
        "    Directory structure is expected as:\n",
        "    data_dir/\n",
        "        split/ (e.g., train/)\n",
        "            class_1/\n",
        "                img1.jpg\n",
        "                img2.jpg\n",
        "            class_2/\n",
        "                ...\n",
        "\n",
        "    Attributes:\n",
        "    - data_dir (str): Root directory containing image data.\n",
        "    - split (str): Subdirectory (e.g., 'train', 'val') to load data from.\n",
        "    - simclr_transform (callable): Transformations to apply to the images (should include data augmentation).\n",
        "    - image_paths (list): Full paths to all images in the specified split.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, split, simclr_transform):\n",
        "        self.split_dir = os.path.join(data_dir, split)\n",
        "        self.simclr_transform = simclr_transform\n",
        "        self.image_paths = []\n",
        "        for cls in os.listdir(self.split_dir):\n",
        "            cls_dir = os.path.join(self.split_dir, cls)\n",
        "            if os.path.isdir(cls_dir):\n",
        "                self.image_paths.extend([os.path.join(cls_dir, img) for img in os.listdir(cls_dir)])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        - int: Total number of images in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Loads an image and applies SimCLR augmentations to generate two distinct views.\n",
        "\n",
        "        Parameters:\n",
        "        - index (int): Index of the image to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: A tuple containing two augmented views of the same image (img1, img2).\n",
        "        \"\"\"\n",
        "        img_path = self.image_paths[index]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img1 = self.simclr_transform(img)\n",
        "        img2 = self.simclr_transform(img)\n",
        "        return img1, img2\n",
        "\n",
        "\n",
        "class SimCLR(nn.Module):\n",
        "    \"\"\"\n",
        "    SimCLR model using a ResNet backbone and a projection head.\n",
        "\n",
        "    The backbone (e.g., ResNet50) is used to extract image features,\n",
        "    and the projection head maps those features into a space suitable for contrastive loss.\n",
        "\n",
        "    Attributes:\n",
        "    - backbone (nn.Module): Feature extractor network with the final classification layer removed.\n",
        "    - projection_head (nn.Sequential): MLP head that projects backbone outputs into a contrastive embedding space.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, projection_dim=128):\n",
        "        \"\"\"\n",
        "        Initializes the SimCLR model.\n",
        "\n",
        "        Parameters:\n",
        "        - backbone (nn.Module): Pretrained ResNet model to use as feature extractor.\n",
        "        - projection_dim (int): Dimensionality of the output projection space.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()  # Remove original classifier\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the SimCLR model.\n",
        "\n",
        "        Parameters:\n",
        "        - x (Tensor): Input image batch of shape (batch_size, 3, H, W).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Projected features of shape (batch_size, projection_dim).\n",
        "        \"\"\"\n",
        "        h = self.backbone(x)\n",
        "        z = self.projection_head(h)\n",
        "        return z\n",
        "\n",
        "def nt_xent_loss(z, tau=0.5):\n",
        "    \"\"\"\n",
        "    Computes the Normalized Temperature-scaled Cross Entropy (NT-Xent) loss used in SimCLR.\n",
        "\n",
        "    This loss encourages positive pairs (two augmented views of the same image) to have similar representations\n",
        "    while pushing apart representations of all other (negative) pairs within the batch.\n",
        "\n",
        "    Assumes the input tensor `z` contains 2N feature vectors, where the first N and second N\n",
        "    are corresponding positive pairs (i.e., for each i in [0, N), z[i] and z[i+N] are a positive pair).\n",
        "\n",
        "    Parameters:\n",
        "    - z (Tensor): A tensor of shape (2N, D), where D is the embedding dimension. Contains projections for all views.\n",
        "    - tau (float): Temperature scaling factor used to soften the distribution in the softmax.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: The scalar NT-Xent loss value.\n",
        "    \"\"\"\n",
        "    z = F.normalize(z, dim=1)  # Normalize embeddings to unit vectors\n",
        "    sim_matrix = torch.mm(z, z.t()) / tau  # Compute pairwise cosine similarities\n",
        "\n",
        "    batch_size = z.size(0) // 2  # N\n",
        "    pos_indices = torch.arange(batch_size, device=z.device)\n",
        "    pos_indices = torch.cat([pos_indices + batch_size, pos_indices])  # Indices of positive pairs\n",
        "\n",
        "    log_softmax = F.log_softmax(sim_matrix, dim=1)\n",
        "    pos_sim = log_softmax[torch.arange(2 * batch_size), pos_indices]  # Log-prob of positives\n",
        "\n",
        "    loss = -pos_sim.mean()  # Mean of negative log-likelihoods for positives\n",
        "    return loss\n",
        "\n",
        "\n",
        "# SimCLR transform\n",
        "simclr_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.2, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6K6XJST_F1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CutMix and Fine-tuning Components ---\n",
        "\n",
        "class CustomClassificationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for image classification tasks using provided image paths and labels.\n",
        "\n",
        "    Each sample in the dataset is an image-label pair, optionally transformed using a provided transform.\n",
        "\n",
        "    Attributes:\n",
        "    - image_paths (list): List of file paths to image files.\n",
        "    - labels (list): List of corresponding class labels for each image.\n",
        "    - transform (callable, optional): Optional transformation to apply to each image.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        - int: Number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Loads an image and its corresponding label by index.\n",
        "\n",
        "        Parameters:\n",
        "        - index (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: A tuple (image, label), where the image may be transformed.\n",
        "        \"\"\"\n",
        "        img_path = self.image_paths[index]\n",
        "        label = self.labels[index]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "def cutmix(data, targets, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Applies CutMix augmentation on a batch of images and their labels.\n",
        "\n",
        "    CutMix replaces a random region of each image with a patch from another image\n",
        "    and mixes the corresponding labels proportionally.\n",
        "\n",
        "    Parameters:\n",
        "    - data (Tensor): A batch of images of shape (B, C, H, W).\n",
        "    - targets (Tensor): Corresponding labels of shape (B,).\n",
        "    - alpha (float): Hyperparameter for the Beta distribution used to sample the mixing ratio.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Augmented images and a tuple of (original_targets, mixed_targets, lambda).\n",
        "    \"\"\"\n",
        "    indices = torch.randperm(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets = targets[indices]\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
        "    data[:, :, bby1:bby2, bbx1:bbx2] = shuffled_data[:, :, bby1:bby2, bbx1:bbx2]\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size(-1) * data.size(-2)))\n",
        "    return data, (targets, shuffled_targets, lam)\n",
        "\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    \"\"\"\n",
        "    Generates a random bounding box for CutMix based on the lambda value.\n",
        "\n",
        "    Parameters:\n",
        "    - size (tuple): Size of the input tensor, expected to be (B, C, H, W).\n",
        "    - lam (float): Lambda value sampled from a Beta distribution for determining cutout area.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Coordinates of the bounding box (bbx1, bby1, bbx2, bby2).\n",
        "    \"\"\"\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Image classification model using a ResNet50 backbone with a custom classification head.\n",
        "\n",
        "    The ResNet50 backbone is used for feature extraction (with the final classification layer removed),\n",
        "    and a new linear layer is added to map the extracted features to the desired number of classes.\n",
        "\n",
        "    Attributes:\n",
        "    - backbone (nn.Module): ResNet50 feature extractor with the final layer removed.\n",
        "    - classifier (nn.Linear): Linear classification layer mapping features to class logits.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, num_classes):\n",
        "        \"\"\"\n",
        "        Initializes the classifier model.\n",
        "\n",
        "        Parameters:\n",
        "        - backbone (nn.Module): Pretrained ResNet50 model to be used as the backbone.\n",
        "        - num_classes (int): Number of output classes.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.classifier = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the classifier.\n",
        "\n",
        "        Parameters:\n",
        "        - x (Tensor): Input image batch of shape (B, 3, H, W).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Logits for each class of shape (B, num_classes).\n",
        "        \"\"\"\n",
        "        features = self.backbone(x)\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n",
        "# Base transform for fine-tuning and evaluation\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "9c_4enLl_LVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Dataset Preparation ---\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/Computer vision with few shot sampling focus group/data_set\"  # Your dataset path\n",
        "\n",
        "# SimCLR Pretraining Dataset\n",
        "simclr_dataset = SimCLRDataset(data_dir, split='train', simclr_transform=simclr_transform)\n",
        "simclr_loader = DataLoader(simclr_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "# Split 'test' split for fine-tuning and evaluation\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "test_classes = [c for c in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, c))]\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(test_classes)}\n",
        "\n",
        "fine_tune_image_paths = []\n",
        "fine_tune_labels = []\n",
        "eval_image_paths = []\n",
        "eval_labels = []\n",
        "\n",
        "for cls in test_classes:\n",
        "    cls_dir = os.path.join(test_dir, cls)\n",
        "    images = [os.path.join(cls_dir, img) for img in os.listdir(cls_dir)]\n",
        "    np.random.shuffle(images)\n",
        "    split_idx = int(0.8 * len(images))  # 80% for fine-tuning, 20% for evaluation\n",
        "    fine_tune_image_paths.extend(images[:split_idx])\n",
        "    fine_tune_labels.extend([class_to_idx[cls]] * split_idx)\n",
        "    eval_image_paths.extend(images[split_idx:])\n",
        "    eval_labels.extend([class_to_idx[cls]] * (len(images) - split_idx))\n",
        "\n",
        "fine_tune_dataset = CustomClassificationDataset(fine_tune_image_paths, fine_tune_labels, transform=base_transform)\n",
        "eval_dataset = CustomClassificationDataset(eval_image_paths, eval_labels, transform=base_transform)\n",
        "\n",
        "fine_tune_loader = DataLoader(fine_tune_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "QaCV9CsO_PAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SimCLR Pretraining ---\n",
        "\n",
        "print(\"Starting SimCLR Pre-training...\")\n",
        "backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "simclr_model = SimCLR(backbone).to(device)\n",
        "optimizer = torch.optim.Adam(simclr_model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    simclr_model.train()\n",
        "    total_loss = 0\n",
        "    for img1, img2 in simclr_loader:\n",
        "        img1, img2 = img1.to(device), img2.to(device)\n",
        "        z1 = simclr_model(img1)\n",
        "        z2 = simclr_model(img2)\n",
        "        z = torch.cat([z1, z2], dim=0)\n",
        "        loss = nt_xent_loss(z)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(simclr_loader)\n",
        "    print(f\"SimCLR Pretraining Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "torch.save(backbone.state_dict(), \"simclr_backbone.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8sbqbQ4_V2O",
        "outputId": "421f957f-2888-4dd3-ad79-795d15dc8f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting SimCLR Pre-training...\n",
            "SimCLR Pretraining Epoch 1/10, Loss: 4.0404\n",
            "SimCLR Pretraining Epoch 2/10, Loss: 3.8645\n",
            "SimCLR Pretraining Epoch 3/10, Loss: 3.8318\n",
            "SimCLR Pretraining Epoch 4/10, Loss: 3.7105\n",
            "SimCLR Pretraining Epoch 5/10, Loss: 3.6864\n",
            "SimCLR Pretraining Epoch 6/10, Loss: 3.6007\n",
            "SimCLR Pretraining Epoch 7/10, Loss: 3.5808\n",
            "SimCLR Pretraining Epoch 8/10, Loss: 3.5730\n",
            "SimCLR Pretraining Epoch 9/10, Loss: 3.5850\n",
            "SimCLR Pretraining Epoch 10/10, Loss: 3.5598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Fine-tuning with CutMix ---\n",
        "\n",
        "print(\"Starting Fine-tuning with CutMix...\")\n",
        "backbone = models.resnet50(weights=None) # Initialize without pre-trained weights\n",
        "backbone.load_state_dict(torch.load(\"simclr_backbone.pth\"), strict=False) # Load with strict=False\n",
        "classifier = Classifier(backbone, num_classes=len(test_classes)).to(device)\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0001)  # Lower LR for fine-tuning\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    for data, targets in fine_tune_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        if np.random.rand() < 0.5:  # Apply CutMix 50% of the time\n",
        "            data, (targets_a, targets_b, lam) = cutmix(data, targets)\n",
        "            logits = classifier(data)\n",
        "            loss = lam * criterion(logits, targets_a) + (1 - lam) * criterion(logits, targets_b)\n",
        "        else:\n",
        "            logits = classifier(data)\n",
        "            loss = criterion(logits, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        if isinstance(targets, tuple):  # Handle CutMix case\n",
        "            targets = targets_a  # Use primary targets for accuracy\n",
        "        total_acc += (preds == targets).float().mean().item()\n",
        "    avg_loss = total_loss / len(fine_tune_loader)\n",
        "    avg_acc = total_acc / len(fine_tune_loader)\n",
        "    print(f\"Fine-tuning Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Acc: {avg_acc:.4f}\")\n",
        "\n",
        "torch.save(classifier.state_dict(), \"fine_tuned_model.pth\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1m4slY5_cjg",
        "outputId": "0e390bfe-de06-4f78-a7ae-1d6ac48fed47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Fine-tuning with CutMix...\n",
            "Fine-tuning Epoch 1/15, Loss: 1.3961, Acc: 0.2844\n",
            "Fine-tuning Epoch 2/15, Loss: 1.2086, Acc: 0.4672\n",
            "Fine-tuning Epoch 3/15, Loss: 1.0619, Acc: 0.5547\n",
            "Fine-tuning Epoch 4/15, Loss: 0.9623, Acc: 0.6750\n",
            "Fine-tuning Epoch 5/15, Loss: 0.9100, Acc: 0.6125\n",
            "Fine-tuning Epoch 6/15, Loss: 0.8542, Acc: 0.7203\n",
            "Fine-tuning Epoch 7/15, Loss: 0.8035, Acc: 0.6938\n",
            "Fine-tuning Epoch 8/15, Loss: 0.9759, Acc: 0.6500\n",
            "Fine-tuning Epoch 9/15, Loss: 0.9932, Acc: 0.6484\n",
            "Fine-tuning Epoch 10/15, Loss: 0.5999, Acc: 0.7734\n",
            "Fine-tuning Epoch 11/15, Loss: 0.8872, Acc: 0.6875\n",
            "Fine-tuning Epoch 12/15, Loss: 0.5618, Acc: 0.8953\n",
            "Fine-tuning Epoch 13/15, Loss: 0.6807, Acc: 0.8484\n",
            "Fine-tuning Epoch 14/15, Loss: 0.4753, Acc: 0.8125\n",
            "Fine-tuning Epoch 15/15, Loss: 0.5359, Acc: 0.8625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluation ---\n",
        "\n",
        "print(\"Starting Evaluation...\")\n",
        "classifier.load_state_dict(torch.load(\"fine_tuned_model.pth\"))\n",
        "classifier.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in eval_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        logits = classifier(data)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "accuracy = correct / total\n",
        "print(f\"Evaluation Accuracy on Test Split: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sct0P7aT_eao",
        "outputId": "ae66c21d-7dfa-4dac-eb42-134744496985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Evaluation...\n",
            "Evaluation Accuracy on Test Split: 0.4444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NHmyIr4m_FwF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}