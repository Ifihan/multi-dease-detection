{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UcIJj79qpwG"
      },
      "source": [
        "# Few Shot Sampling of Blood Smear Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC1zPK9NamYN"
      },
      "source": [
        "## Custom Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "75s1YnBzFJWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fByqZWp5vcNG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def resize_and_pad(image, target_size=(320, 320), fill=0):\n",
        "    \"\"\"\n",
        "    Resize an image to fit within the target size while preserving the aspect ratio,\n",
        "    then pad the shorter sides with a constant value to reach the target size.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): Input image.\n",
        "        target_size (tuple): Target size (width, height), default (224, 224).\n",
        "        fill (int or tuple): Padding fill value (e.g., 0 for black, 255 for white).\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: Resized and padded image of size target_size.\n",
        "    \"\"\"\n",
        "    # Get original dimensions\n",
        "    width, height = image.size\n",
        "    target_width, target_height = target_size\n",
        "\n",
        "    # Calculate scaling factor to fit within target size\n",
        "    scale = min(target_width / width, target_height / height)\n",
        "    new_width = int(width * scale)\n",
        "    new_height = int(height * scale)\n",
        "\n",
        "    # Resize image with correct (height, width) order\n",
        "    resized_image = transforms.functional.resize(image, (new_height, new_width))\n",
        "\n",
        "    # Calculate padding to center the image\n",
        "    padding_left = (target_width - new_width) // 2\n",
        "    padding_top = (target_height - new_height) // 2\n",
        "    # Ensure padding adds up exactly to the difference\n",
        "    padding_right = target_width - new_width - padding_left\n",
        "    padding_bottom = target_height - new_height - padding_top\n",
        "\n",
        "    # Apply padding\n",
        "    padded_image = transforms.functional.pad(\n",
        "        resized_image,\n",
        "        padding=(padding_left, padding_top, padding_right, padding_bottom),\n",
        "        fill=fill\n",
        "    )\n",
        "\n",
        "    return padded_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "osiwpi1mvvjG"
      },
      "outputs": [],
      "source": [
        "# Basic Image transformation\n",
        "def get_base_transforms(target_size=(320, 320), use_grayscale=False):\n",
        "    \"\"\"\n",
        "    Returns a composed set of basic image transformations for preprocessing input images.\n",
        "\n",
        "    Parameters:\n",
        "    - target_size (tuple): The desired output size (height, width) of the image after resizing and padding.\n",
        "    - use_grayscale (bool): If True, converts the image to grayscale with 3 channels before applying other transformations.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Optional grayscale conversion with 3 output channels,\n",
        "        - Resizing and padding the image to match the target size,\n",
        "        - Conversion to tensor,\n",
        "        - Normalization using ImageNet mean and standard deviation.\n",
        "    \"\"\"\n",
        "    base_transforms = [\n",
        "        transforms.Lambda(lambda img: resize_and_pad(img, target_size=target_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        "\n",
        "    if use_grayscale:\n",
        "        base_transforms.insert(0, transforms.Grayscale(num_output_channels=3))  # Keep 3 channels for compatibility\n",
        "    return transforms.Compose(base_transforms)\n",
        "\n",
        "\n",
        "# Data augmentation transforms\n",
        "def get_augmentation_transforms():\n",
        "    \"\"\"\n",
        "    Returns a composed set of data augmentation transformations to artificially expand the training dataset.\n",
        "\n",
        "    This function applies a series of random transformations to simulate variations in brightness, contrast, orientation,\n",
        "    and color mode, helping the model generalize better.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Random brightness and contrast adjustment (ColorJitter),\n",
        "        - Random horizontal and vertical flipping,\n",
        "        - Random rotation by up to ±10 degrees,\n",
        "        - Random conversion to grayscale with a 20% probability.\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Simulate lighting/stain variations\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomGrayscale(p=0.2)  # Optional: Randomly apply grayscale as part of augmentation\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YQtpp1OmFV25"
      },
      "outputs": [],
      "source": [
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, num_ways=5, num_support=5,\n",
        "                 num_query=10, num_episodes=100, target_size=(320, 320),\n",
        "                 use_grayscale=False,\n",
        "                 augment=False,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Path to dataset directory\n",
        "            split (str): One of 'train', 'validation', or 'test'\n",
        "            num_ways (int): Number of classes per episode\n",
        "            num_support (int): Number of support samples per class (i.e. number of shots)\n",
        "            num_query (int): Number of query samples per class\n",
        "            num_episodes (int): Number of episodes per epoch\n",
        "            use_grayscale(bool),  Use grayscale or not\n",
        "            augment(bool),        For data augmentation technique\n",
        "        \"\"\"\n",
        "        self.split_dir = os.path.join(data_dir, split)\n",
        "        self.num_ways = num_ways\n",
        "        self.num_support = num_support\n",
        "        self.num_query = num_query\n",
        "        self.num_episodes = num_episodes\n",
        "        base_transform = get_base_transforms(target_size, use_grayscale)\n",
        "        if augment:\n",
        "            augmentation_transform = get_augmentation_transforms()\n",
        "            self.transform = transforms.Compose([augmentation_transform, base_transform])\n",
        "        else:\n",
        "            self.transform = base_transform\n",
        "\n",
        "        # Load class directories and their images\n",
        "        self.classes = [c for c in os.listdir(self.split_dir)\n",
        "                       if os.path.isdir(os.path.join(self.split_dir, c))]\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}  # Map class names to indices\n",
        "        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}  # Map indices to class names\n",
        "        self.class_images = {\n",
        "            c: [os.path.join(self.split_dir, c, img)\n",
        "                for img in os.listdir(os.path.join(self.split_dir, c))]\n",
        "            for c in self.classes\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_episodes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Randomly select N classes for this episode\n",
        "        selected_classes = np.random.choice(self.classes, self.num_ways, replace=False)\n",
        "\n",
        "        support_images = []\n",
        "        support_labels = []\n",
        "        query_images = []\n",
        "        query_labels = []\n",
        "\n",
        "        for label_idx, class_name in enumerate(selected_classes):\n",
        "            all_images = self.class_images[class_name]\n",
        "            if len(all_images) < self.num_support + self.num_query:\n",
        "                raise ValueError(\n",
        "                    f\"Class {class_name} has only {len(all_images)} images. \"\n",
        "                    f\"Need at least {self.num_support + self.num_query}.\"\n",
        "                )\n",
        "\n",
        "            # Randomly select support and query images\n",
        "            selected_indices = np.random.choice(\n",
        "                len(all_images),\n",
        "                self.num_support + self.num_query,\n",
        "                replace=False #True#########################################################################\n",
        "            )\n",
        "            support_paths = [all_images[i] for i in selected_indices[:self.num_support]]\n",
        "            query_paths = [all_images[i] for i in selected_indices[self.num_support:]]\n",
        "\n",
        "            # Load and transform support images\n",
        "            for path in support_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                support_images.append(img)\n",
        "                support_labels.append(label_idx)\n",
        "\n",
        "            # Load and transform query images\n",
        "            for path in query_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                query_images.append(img)\n",
        "                query_labels.append(label_idx)\n",
        "\n",
        "        # Shuffle the support and query sets\n",
        "        support_indices = np.arange(len(support_images))\n",
        "        query_indices = np.arange(len(query_images))\n",
        "        np.random.shuffle(support_indices)\n",
        "        np.random.shuffle(query_indices)\n",
        "\n",
        "        support_images = [support_images[i] for i in support_indices]\n",
        "        support_labels = [support_labels[i] for i in support_indices]\n",
        "        query_images = [query_images[i] for i in query_indices]\n",
        "        query_labels = [query_labels[i] for i in query_indices]\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        support_set = (\n",
        "            torch.stack(support_images),\n",
        "            torch.tensor(support_labels, dtype=torch.long)\n",
        "        )\n",
        "        query_set = (\n",
        "            torch.stack(query_images),\n",
        "            torch.tensor(query_labels, dtype=torch.long)\n",
        "        )\n",
        "        # Store the selected class names for this episode\n",
        "        selected_classes = [str(cls) for cls in selected_classes]\n",
        "        episode_classes = selected_classes\n",
        "\n",
        "        return support_set, query_set, episode_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "paGubE7bFSLp"
      },
      "outputs": [],
      "source": [
        "def get_data_loader(dataset, batch_size=1, shuffle=True):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the dataset.\n",
        "    Note: Batch size should typically be 1 for few-shot learning,\n",
        "    as each episode is a separate task.\n",
        "    \"\"\"\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJNGAZFtcFtR"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Lht4lJsbX9"
      },
      "source": [
        "### Augmentation Based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GASr7PER9sWl",
        "outputId": "9c03d7c2-1dd5-4703-eb78-e775410a283d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XivOOejr7d1T"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_ways = 5\n",
        "num_shots_eval = [1, 5, 10]\n",
        "data_dir = \"/home/ifihan/multi-dease-detection/data_set\"\n",
        "# data_dir = '/content/drive/MyDrive/Computer vision with few shot sampling focus group/data_set'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ1-_8tG8YBt"
      },
      "source": [
        "#### SimCLR and CutMix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y6K6XJST_F1P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "class SimCLRDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for SimCLR pretraining.\n",
        "\n",
        "    This dataset loads images from a specified directory structure and returns two independently augmented\n",
        "    views of the same image, as required by SimCLR for contrastive learning.\n",
        "\n",
        "    Directory structure is expected as:\n",
        "    data_dir/\n",
        "        split/ (e.g., train/)\n",
        "            class_1/\n",
        "                img1.jpg\n",
        "                img2.jpg\n",
        "            class_2/\n",
        "                ...\n",
        "\n",
        "    Attributes:\n",
        "    - data_dir (str): Root directory containing image data.\n",
        "    - split (str): Subdirectory (e.g., 'train', 'val') to load data from.\n",
        "    - simclr_transform (callable): Transformations to apply to the images (should include data augmentation).\n",
        "    - image_paths (list): Full paths to all images in the specified split.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, split, simclr_transform):\n",
        "        self.split_dir = os.path.join(data_dir, split)\n",
        "        self.simclr_transform = simclr_transform\n",
        "        self.image_paths = []\n",
        "        for cls in os.listdir(self.split_dir):\n",
        "            cls_dir = os.path.join(self.split_dir, cls)\n",
        "            if os.path.isdir(cls_dir):\n",
        "                self.image_paths.extend([os.path.join(cls_dir, img) for img in os.listdir(cls_dir)])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        - int: Total number of images in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Loads an image and applies SimCLR augmentations to generate two distinct views.\n",
        "\n",
        "        Parameters:\n",
        "        - index (int): Index of the image to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: A tuple containing two augmented views of the same image (img1, img2).\n",
        "        \"\"\"\n",
        "        img_path = self.image_paths[index]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img1 = self.simclr_transform(img)\n",
        "        img2 = self.simclr_transform(img)\n",
        "        return img1, img2\n",
        "\n",
        "\n",
        "class SimCLR(nn.Module):\n",
        "    \"\"\"\n",
        "    SimCLR model using a ResNet backbone and a projection head.\n",
        "\n",
        "    The backbone (e.g., ResNet50) is used to extract image features,\n",
        "    and the projection head maps those features into a space suitable for contrastive loss.\n",
        "\n",
        "    Attributes:\n",
        "    - backbone (nn.Module): Feature extractor network with the final classification layer removed.\n",
        "    - projection_head (nn.Sequential): MLP head that projects backbone outputs into a contrastive embedding space.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, projection_dim=128):\n",
        "        \"\"\"\n",
        "        Initializes the SimCLR model.\n",
        "\n",
        "        Parameters:\n",
        "        - backbone (nn.Module): Pretrained ResNet model to use as feature extractor.\n",
        "        - projection_dim (int): Dimensionality of the output projection space.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()  # Remove original classifier\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the SimCLR model.\n",
        "\n",
        "        Parameters:\n",
        "        - x (Tensor): Input image batch of shape (batch_size, 3, H, W).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Projected features of shape (batch_size, projection_dim).\n",
        "        \"\"\"\n",
        "        h = self.backbone(x)\n",
        "        z = self.projection_head(h)\n",
        "        return z\n",
        "\n",
        "def nt_xent_loss(z, tau=0.5):\n",
        "    \"\"\"\n",
        "    Computes the Normalized Temperature-scaled Cross Entropy (NT-Xent) loss used in SimCLR.\n",
        "\n",
        "    This loss encourages positive pairs (two augmented views of the same image) to have similar representations\n",
        "    while pushing apart representations of all other (negative) pairs within the batch.\n",
        "\n",
        "    Assumes the input tensor `z` contains 2N feature vectors, where the first N and second N\n",
        "    are corresponding positive pairs (i.e., for each i in [0, N), z[i] and z[i+N] are a positive pair).\n",
        "\n",
        "    Parameters:\n",
        "    - z (Tensor): A tensor of shape (2N, D), where D is the embedding dimension. Contains projections for all views.\n",
        "    - tau (float): Temperature scaling factor used to soften the distribution in the softmax.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: The scalar NT-Xent loss value.\n",
        "    \"\"\"\n",
        "    z = F.normalize(z, dim=1)  # Normalize embeddings to unit vectors\n",
        "    sim_matrix = torch.mm(z, z.t()) / tau  # Compute pairwise cosine similarities\n",
        "\n",
        "    batch_size = z.size(0) // 2  # N\n",
        "    pos_indices = torch.arange(batch_size, device=z.device)\n",
        "    pos_indices = torch.cat([pos_indices + batch_size, pos_indices])  # Indices of positive pairs\n",
        "\n",
        "    log_softmax = F.log_softmax(sim_matrix, dim=1)\n",
        "    pos_sim = log_softmax[torch.arange(2 * batch_size), pos_indices]  # Log-prob of positives\n",
        "\n",
        "    loss = -pos_sim.mean()  # Mean of negative log-likelihoods for positives\n",
        "    return loss\n",
        "\n",
        "\n",
        "# SimCLR transform\n",
        "simclr_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=320, scale=(0.2, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9c_4enLl_LVD"
      },
      "outputs": [],
      "source": [
        "# --- CutMix and Fine-tuning Components ---\n",
        "\n",
        "class CustomClassificationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for image classification tasks using provided image paths and labels.\n",
        "\n",
        "    Each sample in the dataset is an image-label pair, optionally transformed using a provided transform.\n",
        "\n",
        "    Attributes:\n",
        "    - image_paths (list): List of file paths to image files.\n",
        "    - labels (list): List of corresponding class labels for each image.\n",
        "    - transform (callable, optional): Optional transformation to apply to each image.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        - int: Number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Loads an image and its corresponding label by index.\n",
        "\n",
        "        Parameters:\n",
        "        - index (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: A tuple (image, label), where the image may be transformed.\n",
        "        \"\"\"\n",
        "        img_path = self.image_paths[index]\n",
        "        label = self.labels[index]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "def cutmix(data, targets, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Applies CutMix augmentation on a batch of images and their labels.\n",
        "\n",
        "    CutMix replaces a random region of each image with a patch from another image\n",
        "    and mixes the corresponding labels proportionally.\n",
        "\n",
        "    Parameters:\n",
        "    - data (Tensor): A batch of images of shape (B, C, H, W).\n",
        "    - targets (Tensor): Corresponding labels of shape (B,).\n",
        "    - alpha (float): Hyperparameter for the Beta distribution used to sample the mixing ratio.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Augmented images and a tuple of (original_targets, mixed_targets, lambda).\n",
        "    \"\"\"\n",
        "    indices = torch.randperm(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets = targets[indices]\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
        "    data[:, :, bby1:bby2, bbx1:bbx2] = shuffled_data[:, :, bby1:bby2, bbx1:bbx2]\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size(-1) * data.size(-2)))\n",
        "    return data, (targets, shuffled_targets, lam)\n",
        "\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    \"\"\"\n",
        "    Generates a random bounding box for CutMix based on the lambda value.\n",
        "\n",
        "    Parameters:\n",
        "    - size (tuple): Size of the input tensor, expected to be (B, C, H, W).\n",
        "    - lam (float): Lambda value sampled from a Beta distribution for determining cutout area.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Coordinates of the bounding box (bbx1, bby1, bbx2, bby2).\n",
        "    \"\"\"\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Image classification model using a ResNet50 backbone with a custom classification head.\n",
        "\n",
        "    The ResNet50 backbone is used for feature extraction (with the final classification layer removed),\n",
        "    and a new linear layer is added to map the extracted features to the desired number of classes.\n",
        "\n",
        "    Attributes:\n",
        "    - backbone (nn.Module): ResNet50 feature extractor with the final layer removed.\n",
        "    - classifier (nn.Linear): Linear classification layer mapping features to class logits.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, num_classes):\n",
        "        \"\"\"\n",
        "        Initializes the classifier model.\n",
        "\n",
        "        Parameters:\n",
        "        - backbone (nn.Module): Pretrained ResNet50 model to be used as the backbone.\n",
        "        - num_classes (int): Number of output classes.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.classifier = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the classifier.\n",
        "\n",
        "        Parameters:\n",
        "        - x (Tensor): Input image batch of shape (B, 3, H, W).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Logits for each class of shape (B, num_classes).\n",
        "        \"\"\"\n",
        "        features = self.backbone(x)\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n",
        "# Base transform for fine-tuning and evaluation\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((320, 320)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QaCV9CsO_PAA"
      },
      "outputs": [],
      "source": [
        "# --- Dataset Preparation ---\n",
        "\n",
        "# data_dir = \"/content/drive/MyDrive/Computer vision with few shot sampling focus group/data_set\"  # Your dataset path\n",
        "data_dir = \"/home/ifihan/multi-dease-detection/data_set\"\n",
        "\n",
        "# SimCLR Pretraining Dataset\n",
        "simclr_dataset = SimCLRDataset(data_dir, split='train', simclr_transform=simclr_transform)\n",
        "simclr_loader = DataLoader(simclr_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "# Split 'test' split for fine-tuning and evaluation\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "test_classes = [c for c in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, c))]\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(test_classes)}\n",
        "\n",
        "fine_tune_image_paths = []\n",
        "fine_tune_labels = []\n",
        "eval_image_paths = []\n",
        "eval_labels = []\n",
        "\n",
        "for cls in test_classes:\n",
        "    cls_dir = os.path.join(test_dir, cls)\n",
        "    images = [os.path.join(cls_dir, img) for img in os.listdir(cls_dir)]\n",
        "    np.random.shuffle(images)\n",
        "    split_idx = int(0.8 * len(images))  # 80% for fine-tuning, 20% for evaluation\n",
        "    fine_tune_image_paths.extend(images[:split_idx])\n",
        "    fine_tune_labels.extend([class_to_idx[cls]] * split_idx)\n",
        "    eval_image_paths.extend(images[split_idx:])\n",
        "    eval_labels.extend([class_to_idx[cls]] * (len(images) - split_idx))\n",
        "\n",
        "fine_tune_dataset = CustomClassificationDataset(fine_tune_image_paths, fine_tune_labels, transform=base_transform)\n",
        "eval_dataset = CustomClassificationDataset(eval_image_paths, eval_labels, transform=base_transform)\n",
        "\n",
        "fine_tune_loader = DataLoader(fine_tune_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting SimCLR Pre-training…\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 — Loss: 4.0567\n",
            "Epoch 2/100 — Loss: 3.9036\n",
            "Epoch 3/100 — Loss: 3.6921\n",
            "Epoch 4/100 — Loss: 3.5933\n",
            "Epoch 5/100 — Loss: 3.5762\n",
            "Epoch 6/100 — Loss: 3.5464\n",
            "Epoch 7/100 — Loss: 3.5372\n",
            "Epoch 8/100 — Loss: 3.5059\n",
            "Epoch 9/100 — Loss: 3.4847\n",
            "Epoch 10/100 — Loss: 3.4474\n",
            "Epoch 11/100 — Loss: 3.4317\n",
            "Epoch 12/100 — Loss: 3.4243\n",
            "Epoch 13/100 — Loss: 3.4117\n",
            "Epoch 14/100 — Loss: 3.3971\n",
            "Epoch 15/100 — Loss: 3.3872\n",
            "Epoch 16/100 — Loss: 3.3683\n",
            "Epoch 17/100 — Loss: 3.3686\n",
            "Epoch 18/100 — Loss: 3.3993\n",
            "Epoch 19/100 — Loss: 3.3830\n",
            "Epoch 20/100 — Loss: 3.3521\n",
            "Epoch 21/100 — Loss: 3.3642\n",
            "Epoch 22/100 — Loss: 3.3466\n",
            "Epoch 23/100 — Loss: 3.3515\n",
            "Epoch 24/100 — Loss: 3.3405\n",
            "Epoch 25/100 — Loss: 3.3166\n",
            "  ▶ Saved checkpoint: results/simclr_epoch_25.pth\n",
            "Epoch 26/100 — Loss: 3.3168\n",
            "Epoch 27/100 — Loss: 3.3016\n",
            "Epoch 28/100 — Loss: 3.2931\n",
            "Epoch 29/100 — Loss: 3.3153\n",
            "Epoch 30/100 — Loss: 3.2774\n",
            "Epoch 31/100 — Loss: 3.3175\n",
            "Epoch 32/100 — Loss: 3.2809\n",
            "Epoch 33/100 — Loss: 3.2872\n",
            "Epoch 34/100 — Loss: 3.2999\n",
            "Epoch 35/100 — Loss: 3.2786\n",
            "Epoch 36/100 — Loss: 3.2709\n",
            "Epoch 37/100 — Loss: 3.2802\n",
            "Epoch 38/100 — Loss: 3.2921\n",
            "Epoch 39/100 — Loss: 3.3013\n",
            "Epoch 40/100 — Loss: 3.2859\n",
            "Epoch 41/100 — Loss: 3.2767\n",
            "Epoch 42/100 — Loss: 3.2574\n",
            "Epoch 43/100 — Loss: 3.2709\n",
            "Epoch 44/100 — Loss: 3.2452\n",
            "Epoch 45/100 — Loss: 3.2463\n",
            "Epoch 46/100 — Loss: 3.2411\n",
            "Epoch 47/100 — Loss: 3.2640\n",
            "Epoch 48/100 — Loss: 3.2575\n",
            "Epoch 49/100 — Loss: 3.2489\n",
            "Epoch 50/100 — Loss: 3.2415\n",
            "  ▶ Saved checkpoint: results/simclr_epoch_50.pth\n",
            "Epoch 51/100 — Loss: 3.2380\n",
            "Epoch 52/100 — Loss: 3.2517\n",
            "Epoch 53/100 — Loss: 3.2279\n",
            "Epoch 54/100 — Loss: 3.2256\n",
            "Epoch 55/100 — Loss: 3.2293\n",
            "Epoch 56/100 — Loss: 3.2181\n",
            "Epoch 57/100 — Loss: 3.2280\n",
            "Epoch 58/100 — Loss: 3.2170\n",
            "Epoch 59/100 — Loss: 3.2181\n",
            "Epoch 60/100 — Loss: 3.2394\n",
            "Epoch 61/100 — Loss: 3.2146\n",
            "Epoch 62/100 — Loss: 3.2178\n",
            "Epoch 63/100 — Loss: 3.2124\n",
            "Epoch 64/100 — Loss: 3.2358\n",
            "Epoch 65/100 — Loss: 3.2207\n",
            "Epoch 66/100 — Loss: 3.2282\n",
            "Epoch 67/100 — Loss: 3.2283\n",
            "Epoch 68/100 — Loss: 3.2293\n",
            "Epoch 69/100 — Loss: 3.2339\n",
            "Epoch 70/100 — Loss: 3.2333\n",
            "Epoch 71/100 — Loss: 3.2114\n",
            "Epoch 72/100 — Loss: 3.2159\n",
            "Epoch 73/100 — Loss: 3.2347\n",
            "Epoch 74/100 — Loss: 3.2013\n",
            "Epoch 75/100 — Loss: 3.2141\n",
            "  ▶ Saved checkpoint: results/simclr_epoch_75.pth\n",
            "Epoch 76/100 — Loss: 3.2168\n",
            "Epoch 77/100 — Loss: 3.2020\n",
            "Epoch 78/100 — Loss: 3.2065\n",
            "Epoch 79/100 — Loss: 3.2107\n",
            "Epoch 80/100 — Loss: 3.1910\n",
            "Epoch 81/100 — Loss: 3.1935\n",
            "Epoch 82/100 — Loss: 3.1841\n",
            "Epoch 83/100 — Loss: 3.1912\n",
            "Epoch 84/100 — Loss: 3.1723\n",
            "Epoch 85/100 — Loss: 3.1960\n",
            "Epoch 86/100 — Loss: 3.1973\n",
            "Epoch 87/100 — Loss: 3.1929\n",
            "Epoch 88/100 — Loss: 3.1819\n",
            "Epoch 89/100 — Loss: 3.1900\n",
            "Epoch 90/100 — Loss: 3.1890\n",
            "Epoch 91/100 — Loss: 3.1873\n",
            "Epoch 92/100 — Loss: 3.1619\n",
            "Epoch 93/100 — Loss: 3.1779\n",
            "Epoch 94/100 — Loss: 3.1748\n",
            "Epoch 95/100 — Loss: 3.1799\n",
            "Epoch 96/100 — Loss: 3.1682\n",
            "Epoch 97/100 — Loss: 3.1717\n",
            "Epoch 98/100 — Loss: 3.1769\n",
            "Epoch 99/100 — Loss: 3.1836\n",
            "Epoch 100/100 — Loss: 3.1641\n",
            "  ▶ Saved checkpoint: results/simclr_epoch_100.pth\n",
            "Training complete. Final model saved to: results/simclr_final_model.pth\n"
          ]
        }
      ],
      "source": [
        "results_dir = \"results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "print(\"Starting SimCLR Pre-training…\")\n",
        "backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "simclr_model = SimCLR(backbone).to(device)\n",
        "optimizer = torch.optim.Adam(simclr_model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    simclr_model.train()\n",
        "    total_loss = 0.0\n",
        "    for img1, img2 in simclr_loader:\n",
        "        img1, img2 = img1.to(device), img2.to(device)\n",
        "        z1 = simclr_model(img1)\n",
        "        z2 = simclr_model(img2)\n",
        "        z = torch.cat([z1, z2], dim=0)\n",
        "        loss = nt_xent_loss(z)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(simclr_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save a checkpoint every 25 epochs\n",
        "    if (epoch + 1) % 25 == 0:\n",
        "        ckpt_path = os.path.join(results_dir, f\"simclr_epoch_{epoch+1}.pth\")\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': simclr_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }, ckpt_path)\n",
        "        print(f\"  ▶ Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "# After all epochs, save the final model\n",
        "final_path = os.path.join(results_dir, \"simclr_final_model.pth\")\n",
        "torch.save({\n",
        "    'epoch': num_epochs,\n",
        "    'model_state_dict': simclr_model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': avg_loss,\n",
        "}, final_path)\n",
        "print(f\"Training complete. Final model saved to: {final_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Fine-tuning with CutMix...\n",
            "Epoch 1/15 — Loss: 1.3841, Acc: 0.2703\n",
            "Epoch 2/15 — Loss: 1.4134, Acc: 0.3094\n",
            "Epoch 3/15 — Loss: 1.2853, Acc: 0.4703\n",
            "Epoch 4/15 — Loss: 1.2052, Acc: 0.3984\n",
            "Epoch 5/15 — Loss: 1.1254, Acc: 0.5438\n",
            "  ▶ Saved checkpoint: results/cutmix_epoch_5.pth\n",
            "Epoch 6/15 — Loss: 1.0749, Acc: 0.5750\n",
            "Epoch 7/15 — Loss: 1.1237, Acc: 0.4781\n",
            "Epoch 8/15 — Loss: 1.0472, Acc: 0.5953\n",
            "Epoch 9/15 — Loss: 0.8682, Acc: 0.6922\n",
            "Epoch 10/15 — Loss: 1.0693, Acc: 0.5703\n",
            "  ▶ Saved checkpoint: results/cutmix_epoch_10.pth\n",
            "Epoch 11/15 — Loss: 0.8706, Acc: 0.6359\n",
            "Epoch 12/15 — Loss: 0.9550, Acc: 0.6562\n",
            "Epoch 13/15 — Loss: 0.7208, Acc: 0.7219\n",
            "Epoch 14/15 — Loss: 0.7276, Acc: 0.7109\n",
            "Epoch 15/15 — Loss: 0.7403, Acc: 0.7469\n",
            "  ▶ Saved checkpoint: results/cutmix_epoch_15.pth\n",
            "Fine‑tuning complete. Model saved to: results/fine_tuned_model.pth\n"
          ]
        }
      ],
      "source": [
        "results_dir = \"results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# --- Fine-tuning with CutMix ---\n",
        "print(\"Starting Fine-tuning with CutMix...\")\n",
        "\n",
        "# Initialize backbone and load your SimCLR‐pretrained weights\n",
        "backbone = models.resnet50(weights=None)\n",
        "backbone.load_state_dict(\n",
        "    torch.load(os.path.join(results_dir, \"simclr_final_model.pth\")), \n",
        "    strict=False\n",
        ")\n",
        "\n",
        "# Build classifier\n",
        "classifier = Classifier(backbone, num_classes=len(test_classes)).to(device)\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    classifier.train()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "\n",
        "    for data, targets in fine_tune_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        if np.random.rand() < 0.5:  # 50% CutMix\n",
        "            data, (targets_a, targets_b, lam) = cutmix(data, targets)\n",
        "            logits = classifier(data)\n",
        "            loss = lam * criterion(logits, targets_a) + (1 - lam) * criterion(logits, targets_b)\n",
        "        else:\n",
        "            logits = classifier(data)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        # For accuracy, just use targets_a when CutMix was applied\n",
        "        current_targets = targets_a if isinstance(targets, tuple) else targets\n",
        "        total_acc += (preds == current_targets).float().mean().item()\n",
        "\n",
        "    avg_loss = total_loss / len(fine_tune_loader)\n",
        "    avg_acc  = total_acc  / len(fine_tune_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f}, Acc: {avg_acc:.4f}\")\n",
        "\n",
        "    # (Optional) save intermediate checkpoints, e.g. every 5 epochs:\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_path = os.path.join(results_dir, f\"cutmix_epoch_{epoch+1}.pth\")\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': classifier.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "            'acc': avg_acc\n",
        "        }, ckpt_path)\n",
        "        print(f\"  ▶ Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "# Save final fine‑tuned model\n",
        "final_path = os.path.join(results_dir, \"fine_tuned_model.pth\")\n",
        "torch.save({\n",
        "    'epoch': num_epochs,\n",
        "    'model_state_dict': classifier.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': avg_loss,\n",
        "    'acc': avg_acc\n",
        "}, final_path)\n",
        "print(f\"Fine‑tuning complete. Model saved to: {final_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sct0P7aT_eao",
        "outputId": "ae66c21d-7dfa-4dac-eb42-134744496985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Evaluation...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Accuracy on Test Split: 0.2963\n"
          ]
        }
      ],
      "source": [
        "# --- Evaluation ---\n",
        "\n",
        "results_dir = \"results\"\n",
        "print(\"Starting Evaluation...\")\n",
        "\n",
        "# Load the saved checkpoint dict\n",
        "ckpt = torch.load(\n",
        "    os.path.join(results_dir, \"fine_tuned_model.pth\"),\n",
        "    map_location=device\n",
        ")\n",
        "\n",
        "# Load only the model weights\n",
        "classifier.load_state_dict(ckpt['model_state_dict'])\n",
        "classifier.to(device)\n",
        "classifier.eval()\n",
        "\n",
        "correct = 0\n",
        "total   = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in eval_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        logits = classifier(data)\n",
        "        preds  = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == targets).sum().item()\n",
        "        total   += targets.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Evaluation Accuracy on Test Split: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHmyIr4m_FwF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FC1zPK9NamYN",
        "ZJ1-_8tG8YBt",
        "dDYFGE-JsMuT"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
