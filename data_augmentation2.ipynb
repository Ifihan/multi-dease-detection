{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UcIJj79qpwG"
      },
      "source": [
        "# Few Shot Sampling of Blood Smear Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC1zPK9NamYN"
      },
      "source": [
        "## Custom Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75s1YnBzFJWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fByqZWp5vcNG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def resize_and_pad(image, target_size=(320, 320), fill=0):\n",
        "    \"\"\"\n",
        "    Resize an image to fit within the target size while preserving the aspect ratio,\n",
        "    then pad the shorter sides with a constant value to reach the target size.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): Input image.\n",
        "        target_size (tuple): Target size (width, height), default (224, 224).\n",
        "        fill (int or tuple): Padding fill value (e.g., 0 for black, 255 for white).\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: Resized and padded image of size target_size.\n",
        "    \"\"\"\n",
        "    # Get original dimensions\n",
        "    width, height = image.size\n",
        "    target_width, target_height = target_size\n",
        "\n",
        "    # Calculate scaling factor to fit within target size\n",
        "    scale = min(target_width / width, target_height / height)\n",
        "    new_width = int(width * scale)\n",
        "    new_height = int(height * scale)\n",
        "\n",
        "    # Resize image with correct (height, width) order\n",
        "    resized_image = transforms.functional.resize(image, (new_height, new_width))\n",
        "\n",
        "    # Calculate padding to center the image\n",
        "    padding_left = (target_width - new_width) // 2\n",
        "    padding_top = (target_height - new_height) // 2\n",
        "    # Ensure padding adds up exactly to the difference\n",
        "    padding_right = target_width - new_width - padding_left\n",
        "    padding_bottom = target_height - new_height - padding_top\n",
        "\n",
        "    # Apply padding\n",
        "    padded_image = transforms.functional.pad(\n",
        "        resized_image,\n",
        "        padding=(padding_left, padding_top, padding_right, padding_bottom),\n",
        "        fill=fill\n",
        "    )\n",
        "\n",
        "    return padded_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osiwpi1mvvjG"
      },
      "outputs": [],
      "source": [
        "# Basic Image transformation\n",
        "def get_base_transforms(target_size=(320, 320), use_grayscale=False):\n",
        "    \"\"\"\n",
        "    Returns a composed set of basic image transformations for preprocessing input images.\n",
        "\n",
        "    Parameters:\n",
        "    - target_size (tuple): The desired output size (height, width) of the image after resizing and padding.\n",
        "    - use_grayscale (bool): If True, converts the image to grayscale with 3 channels before applying other transformations.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Optional grayscale conversion with 3 output channels,\n",
        "        - Resizing and padding the image to match the target size,\n",
        "        - Conversion to tensor,\n",
        "        - Normalization using ImageNet mean and standard deviation.\n",
        "    \"\"\"\n",
        "    base_transforms = [\n",
        "        transforms.Lambda(lambda img: resize_and_pad(img, target_size=target_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        "\n",
        "    if use_grayscale:\n",
        "        base_transforms.insert(0, transforms.Grayscale(num_output_channels=3))  # Keep 3 channels for compatibility\n",
        "    return transforms.Compose(base_transforms)\n",
        "\n",
        "\n",
        "# Data augmentation transforms\n",
        "def get_augmentation_transforms():\n",
        "    \"\"\"\n",
        "    Returns a composed set of data augmentation transformations to artificially expand the training dataset.\n",
        "\n",
        "    This function applies a series of random transformations to simulate variations in brightness, contrast, orientation,\n",
        "    and color mode, helping the model generalize better.\n",
        "\n",
        "    Returns:\n",
        "    - torchvision.transforms.Compose: A sequence of transformations including:\n",
        "        - Random brightness and contrast adjustment (ColorJitter),\n",
        "        - Random horizontal and vertical flipping,\n",
        "        - Random rotation by up to ±10 degrees,\n",
        "        - Random conversion to grayscale with a 20% probability.\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Simulate lighting/stain variations\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomGrayscale(p=0.2)  # Optional: Randomly apply grayscale as part of augmentation\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQtpp1OmFV25"
      },
      "outputs": [],
      "source": [
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, num_ways=5, num_support=5,\n",
        "                 num_query=10, num_episodes=100, target_size=(320, 320),\n",
        "                 use_grayscale=False,\n",
        "                 augment=False,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Path to dataset directory\n",
        "            split (str): One of 'train', 'validation', or 'test'\n",
        "            num_ways (int): Number of classes per episode\n",
        "            num_support (int): Number of support samples per class (i.e. number of shots)\n",
        "            num_query (int): Number of query samples per class\n",
        "            num_episodes (int): Number of episodes per epoch\n",
        "            use_grayscale(bool),  Use grayscale or not\n",
        "            augment(bool),        For data augmentation technique\n",
        "        \"\"\"\n",
        "        self.split_dir = os.path.join(data_dir, split)\n",
        "        self.num_ways = num_ways\n",
        "        self.num_support = num_support\n",
        "        self.num_query = num_query\n",
        "        self.num_episodes = num_episodes\n",
        "        base_transform = get_base_transforms(target_size, use_grayscale)\n",
        "        if augment:\n",
        "            augmentation_transform = get_augmentation_transforms()\n",
        "            self.transform = transforms.Compose([augmentation_transform, base_transform])\n",
        "        else:\n",
        "            self.transform = base_transform\n",
        "\n",
        "        # Load class directories and their images\n",
        "        self.classes = [c for c in os.listdir(self.split_dir)\n",
        "                       if os.path.isdir(os.path.join(self.split_dir, c))]\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}  # Map class names to indices\n",
        "        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}  # Map indices to class names\n",
        "        self.class_images = {\n",
        "            c: [os.path.join(self.split_dir, c, img)\n",
        "                for img in os.listdir(os.path.join(self.split_dir, c))]\n",
        "            for c in self.classes\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_episodes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Randomly select N classes for this episode\n",
        "        selected_classes = np.random.choice(self.classes, self.num_ways, replace=False)\n",
        "\n",
        "        support_images = []\n",
        "        support_labels = []\n",
        "        query_images = []\n",
        "        query_labels = []\n",
        "\n",
        "        for label_idx, class_name in enumerate(selected_classes):\n",
        "            all_images = self.class_images[class_name]\n",
        "            if len(all_images) < self.num_support + self.num_query:\n",
        "                raise ValueError(\n",
        "                    f\"Class {class_name} has only {len(all_images)} images. \"\n",
        "                    f\"Need at least {self.num_support + self.num_query}.\"\n",
        "                )\n",
        "\n",
        "            # Randomly select support and query images\n",
        "            selected_indices = np.random.choice(\n",
        "                len(all_images),\n",
        "                self.num_support + self.num_query,\n",
        "                replace=False #True#########################################################################\n",
        "            )\n",
        "            support_paths = [all_images[i] for i in selected_indices[:self.num_support]]\n",
        "            query_paths = [all_images[i] for i in selected_indices[self.num_support:]]\n",
        "\n",
        "            # Load and transform support images\n",
        "            for path in support_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                support_images.append(img)\n",
        "                support_labels.append(label_idx)\n",
        "\n",
        "            # Load and transform query images\n",
        "            for path in query_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                query_images.append(img)\n",
        "                query_labels.append(label_idx)\n",
        "\n",
        "        # Shuffle the support and query sets\n",
        "        support_indices = np.arange(len(support_images))\n",
        "        query_indices = np.arange(len(query_images))\n",
        "        np.random.shuffle(support_indices)\n",
        "        np.random.shuffle(query_indices)\n",
        "\n",
        "        support_images = [support_images[i] for i in support_indices]\n",
        "        support_labels = [support_labels[i] for i in support_indices]\n",
        "        query_images = [query_images[i] for i in query_indices]\n",
        "        query_labels = [query_labels[i] for i in query_indices]\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        support_set = (\n",
        "            torch.stack(support_images),\n",
        "            torch.tensor(support_labels, dtype=torch.long)\n",
        "        )\n",
        "        query_set = (\n",
        "            torch.stack(query_images),\n",
        "            torch.tensor(query_labels, dtype=torch.long)\n",
        "        )\n",
        "        # Store the selected class names for this episode\n",
        "        selected_classes = [str(cls) for cls in selected_classes]\n",
        "        episode_classes = selected_classes\n",
        "\n",
        "        return support_set, query_set, episode_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paGubE7bFSLp"
      },
      "outputs": [],
      "source": [
        "def get_data_loader(dataset, batch_size=1, shuffle=True):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the dataset.\n",
        "    Note: Batch size should typically be 1 for few-shot learning,\n",
        "    as each episode is a separate task.\n",
        "    \"\"\"\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJNGAZFtcFtR"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Lht4lJsbX9"
      },
      "source": [
        "### Augmentation Based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ1-_8tG8YBt"
      },
      "source": [
        "#### SimCLR and CutMix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y6K6XJST_F1P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class SimCLRDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for SimCLR pretraining.\n",
        "\n",
        "    This dataset loads images from a specified directory structure and returns two independently augmented\n",
        "    views of the same image, as required by SimCLR for contrastive learning.\n",
        "\n",
        "    Directory structure is expected as:\n",
        "    data_dir/\n",
        "        split/ (e.g., train/)\n",
        "            class_1/\n",
        "                img1.jpg\n",
        "                img2.jpg\n",
        "            class_2/\n",
        "                ...\n",
        "\n",
        "    Attributes:\n",
        "    - data_dir (str): Root directory containing image data.\n",
        "    - split (str): Subdirectory (e.g., 'train', 'val') to load data from.\n",
        "    - simclr_transform (callable): Transformations to apply to the images (should include data augmentation).\n",
        "    - image_paths (list): Full paths to all images in the specified split.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, split, simclr_transform):\n",
        "        self.split_dir = os.path.join(data_dir, split)\n",
        "        self.simclr_transform = simclr_transform\n",
        "        self.image_paths = []\n",
        "        for cls in os.listdir(self.split_dir):\n",
        "            cls_dir = os.path.join(self.split_dir, cls)\n",
        "            if os.path.isdir(cls_dir):\n",
        "                self.image_paths.extend([os.path.join(cls_dir, img) for img in os.listdir(cls_dir)])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        - int: Total number of images in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Loads an image and applies SimCLR augmentations to generate two distinct views.\n",
        "\n",
        "        Parameters:\n",
        "        - index (int): Index of the image to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: A tuple containing two augmented views of the same image (img1, img2).\n",
        "        \"\"\"\n",
        "        img_path = self.image_paths[index]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img1 = self.simclr_transform(img)\n",
        "        img2 = self.simclr_transform(img)\n",
        "        return img1, img2\n",
        "\n",
        "\n",
        "class SimCLR(nn.Module):\n",
        "    \"\"\"\n",
        "    SimCLR model using a ResNet backbone and a projection head.\n",
        "\n",
        "    The backbone (e.g., ResNet50) is used to extract image features,\n",
        "    and the projection head maps those features into a space suitable for contrastive loss.\n",
        "\n",
        "    Attributes:\n",
        "    - backbone (nn.Module): Feature extractor network with the final classification layer removed.\n",
        "    - projection_head (nn.Sequential): MLP head that projects backbone outputs into a contrastive embedding space.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, projection_dim=128):\n",
        "        \"\"\"\n",
        "        Initializes the SimCLR model.\n",
        "\n",
        "        Parameters:\n",
        "        - backbone (nn.Module): Pretrained ResNet model to use as feature extractor.\n",
        "        - projection_dim (int): Dimensionality of the output projection space.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()  # Remove original classifier\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the SimCLR model.\n",
        "\n",
        "        Parameters:\n",
        "        - x (Tensor): Input image batch of shape (batch_size, 3, H, W).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Projected features of shape (batch_size, projection_dim).\n",
        "        \"\"\"\n",
        "        h = self.backbone(x)\n",
        "        z = self.projection_head(h)\n",
        "        return z\n",
        "\n",
        "def nt_xent_loss(z, tau=0.5):\n",
        "    \"\"\"\n",
        "    Computes the Normalized Temperature-scaled Cross Entropy (NT-Xent) loss used in SimCLR.\n",
        "\n",
        "    This loss encourages positive pairs (two augmented views of the same image) to have similar representations\n",
        "    while pushing apart representations of all other (negative) pairs within the batch.\n",
        "\n",
        "    Assumes the input tensor `z` contains 2N feature vectors, where the first N and second N\n",
        "    are corresponding positive pairs (i.e., for each i in [0, N), z[i] and z[i+N] are a positive pair).\n",
        "\n",
        "    Parameters:\n",
        "    - z (Tensor): A tensor of shape (2N, D), where D is the embedding dimension. Contains projections for all views.\n",
        "    - tau (float): Temperature scaling factor used to soften the distribution in the softmax.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: The scalar NT-Xent loss value.\n",
        "    \"\"\"\n",
        "    z = F.normalize(z, dim=1)  # Normalize embeddings to unit vectors\n",
        "    sim_matrix = torch.mm(z, z.t()) / tau  # Compute pairwise cosine similarities\n",
        "\n",
        "    batch_size = z.size(0) // 2  # N\n",
        "    pos_indices = torch.arange(batch_size, device=z.device)\n",
        "    pos_indices = torch.cat([pos_indices + batch_size, pos_indices])  # Indices of positive pairs\n",
        "\n",
        "    log_softmax = F.log_softmax(sim_matrix, dim=1)\n",
        "    pos_sim = log_softmax[torch.arange(2 * batch_size), pos_indices]  # Log-prob of positives\n",
        "\n",
        "    loss = -pos_sim.mean()  # Mean of negative log-likelihoods for positives\n",
        "    return loss\n",
        "\n",
        "\n",
        "# SimCLR transform\n",
        "simclr_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=320, scale=(0.2, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9c_4enLl_LVD"
      },
      "outputs": [],
      "source": [
        "# --- CutMix and Fine-tuning Components ---\n",
        "\n",
        "class CustomClassificationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for image classification tasks using provided image paths and labels.\n",
        "\n",
        "    Each sample in the dataset is an image-label pair, optionally transformed using a provided transform.\n",
        "\n",
        "    Attributes:\n",
        "    - image_paths (list): List of file paths to image files.\n",
        "    - labels (list): List of corresponding class labels for each image.\n",
        "    - transform (callable, optional): Optional transformation to apply to each image.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        - int: Number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Loads an image and its corresponding label by index.\n",
        "\n",
        "        Parameters:\n",
        "        - index (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: A tuple (image, label), where the image may be transformed.\n",
        "        \"\"\"\n",
        "        img_path = self.image_paths[index]\n",
        "        label = self.labels[index]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "def cutmix(data, targets, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Applies CutMix augmentation on a batch of images and their labels.\n",
        "\n",
        "    CutMix replaces a random region of each image with a patch from another image\n",
        "    and mixes the corresponding labels proportionally.\n",
        "\n",
        "    Parameters:\n",
        "    - data (Tensor): A batch of images of shape (B, C, H, W).\n",
        "    - targets (Tensor): Corresponding labels of shape (B,).\n",
        "    - alpha (float): Hyperparameter for the Beta distribution used to sample the mixing ratio.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Augmented images and a tuple of (original_targets, mixed_targets, lambda).\n",
        "    \"\"\"\n",
        "    indices = torch.randperm(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets = targets[indices]\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
        "    data[:, :, bby1:bby2, bbx1:bbx2] = shuffled_data[:, :, bby1:bby2, bbx1:bbx2]\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size(-1) * data.size(-2)))\n",
        "    return data, (targets, shuffled_targets, lam)\n",
        "\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    \"\"\"\n",
        "    Generates a random bounding box for CutMix based on the lambda value.\n",
        "\n",
        "    Parameters:\n",
        "    - size (tuple): Size of the input tensor, expected to be (B, C, H, W).\n",
        "    - lam (float): Lambda value sampled from a Beta distribution for determining cutout area.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Coordinates of the bounding box (bbx1, bby1, bbx2, bby2).\n",
        "    \"\"\"\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Image classification model using a ResNet50 backbone with a custom classification head.\n",
        "\n",
        "    The ResNet50 backbone is used for feature extraction (with the final classification layer removed),\n",
        "    and a new linear layer is added to map the extracted features to the desired number of classes.\n",
        "\n",
        "    Attributes:\n",
        "    - backbone (nn.Module): ResNet50 feature extractor with the final layer removed.\n",
        "    - classifier (nn.Linear): Linear classification layer mapping features to class logits.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, num_classes):\n",
        "        \"\"\"\n",
        "        Initializes the classifier model.\n",
        "\n",
        "        Parameters:\n",
        "        - backbone (nn.Module): Pretrained ResNet50 model to be used as the backbone.\n",
        "        - num_classes (int): Number of output classes.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.classifier = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the classifier.\n",
        "\n",
        "        Parameters:\n",
        "        - x (Tensor): Input image batch of shape (B, 3, H, W).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Logits for each class of shape (B, num_classes).\n",
        "        \"\"\"\n",
        "        features = self.backbone(x)\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n",
        "# Base transform for fine-tuning and evaluation\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((320, 320)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GASr7PER9sWl",
        "outputId": "9cdf1435-dad4-4cb4-c16a-73eea69e7738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XivOOejr7d1T"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_ways = 5\n",
        "num_shots_eval = [1, 5, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rX4l5L55OdlC"
      },
      "outputs": [],
      "source": [
        "# --- Dataset Preparation ---\n",
        "data_dir = \"/content/drive/MyDrive/Computer vision with few shot sampling focus group/data_set\"\n",
        "#data_dir = \"/home/ifihan/multi-dease-detection/data_set\"\n",
        "results_dir = \"results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# SimCLR Pre-training Dataset\n",
        "simclr_dataset = SimCLRDataset(data_dir, split='train', simclr_transform=simclr_transform)\n",
        "\n",
        "# Split 'train' dataset into train and validation for pre-training\n",
        "train_size = int(0.9 * len(simclr_dataset))\n",
        "val_size = len(simclr_dataset) - train_size\n",
        "simclr_train_dataset, simclr_val_dataset = random_split(simclr_dataset, [train_size, val_size])\n",
        "simclr_train_loader = DataLoader(simclr_train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "simclr_val_loader = DataLoader(simclr_val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Split 'test' dataset for fine-tuning and evaluation\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "test_classes = [c for c in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, c))]\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(test_classes)}\n",
        "\n",
        "fine_tune_image_paths = []\n",
        "fine_tune_labels = []\n",
        "eval_image_paths = []\n",
        "eval_labels = []\n",
        "\n",
        "for cls in test_classes:\n",
        "    cls_dir = os.path.join(test_dir, cls)\n",
        "    images = [os.path.join(cls_dir, img) for img in os.listdir(cls_dir)]\n",
        "    np.random.shuffle(images)\n",
        "    split_idx = int(0.7 * len(images))  # 70% for fine-tuning, 30% for evaluation\n",
        "    fine_tune_image_paths.extend(images[:split_idx])\n",
        "    fine_tune_labels.extend([class_to_idx[cls]] * split_idx)\n",
        "    eval_image_paths.extend(images[split_idx:])\n",
        "    eval_labels.extend([class_to_idx[cls]] * (len(images) - split_idx))\n",
        "\n",
        "fine_tune_dataset = CustomClassificationDataset(fine_tune_image_paths, fine_tune_labels, transform=base_transform)\n",
        "eval_dataset = CustomClassificationDataset(eval_image_paths, eval_labels, transform=base_transform)\n",
        "\n",
        "# Further split fine_tune_dataset into train and val for fine-tuning\n",
        "ft_train_size = int(0.8 * len(fine_tune_dataset))\n",
        "ft_val_size = len(fine_tune_dataset) - ft_train_size\n",
        "ft_train_dataset, ft_val_dataset = random_split(fine_tune_dataset, [ft_train_size, ft_val_size])\n",
        "ft_train_loader = DataLoader(ft_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "ft_val_loader = DataLoader(ft_val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4WwfuP4QwaQC"
      },
      "outputs": [],
      "source": [
        "backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "simclr_model = SimCLR(backbone).to(device)\n",
        "optimizer = optim.Adam(simclr_model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "4_M3tCcjMzN9",
        "outputId": "468f70d0-713c-4c05-fbf5-00ce4da6432f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting SimCLR Pre-training…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 - Train: 100%|██████████| 25/25 [05:12<00:00, 12.52s/it]\n",
            "Epoch 1/100 - Val: 100%|██████████| 3/3 [00:40<00:00, 13.41s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 — Train Loss: 3.5623, Val Loss: 3.7342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/100 - Train:  44%|████▍     | 11/25 [01:28<01:52,  8.01s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-aa55c3705e41>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimclr_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "results_dir = \"results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "num_epochs = 100\n",
        "patience = 10\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"Starting SimCLR Pre-training…\")\n",
        "for epoch in range(num_epochs):\n",
        "    simclr_model.train()\n",
        "    total_train_loss = 0.0\n",
        "    for img1, img2 in tqdm(simclr_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\"):\n",
        "        img1, img2 = img1.to(device), img2.to(device)\n",
        "        z1 = simclr_model(img1)\n",
        "        z2 = simclr_model(img2)\n",
        "        z = torch.cat([z1, z2], dim=0)\n",
        "        loss = nt_xent_loss(z)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "    avg_train_loss = total_train_loss / len(simclr_train_loader)\n",
        "\n",
        "    simclr_model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for img1, img2 in tqdm(simclr_val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\"):\n",
        "            img1, img2 = img1.to(device), img2.to(device)\n",
        "            z1 = simclr_model(img1)\n",
        "            z2 = simclr_model(img2)\n",
        "            z = torch.cat([z1, z2], dim=0)\n",
        "            loss = nt_xent_loss(z)\n",
        "            total_val_loss += loss.item()\n",
        "    avg_val_loss = total_val_loss / len(simclr_val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} — Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': simclr_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_val_loss,\n",
        "        }, os.path.join(results_dir, \"simclr_best_model.pth\"))\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Plot pre-training losses\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('SimCLR Pre-training Losses')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(results_dir, \"simclr_losses.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(f\"Pre-training complete. Best model saved to: {os.path.join(results_dir, 'simclr_best_model.pth')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA459MkwN_Pa",
        "outputId": "893c985b-14fc-46fc-ed19-07150736b84c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Fine-tuning with CutMix...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 - Train: 100%|██████████| 3/3 [00:20<00:00,  6.97s/it]\n",
            "Epoch 1/100 - Val: 100%|██████████| 1/1 [00:13<00:00, 13.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 — Train Loss: 1.7530, Train Acc: 0.2438, Val Loss: 1.4030, Val Acc: 0.1579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/100 - Train: 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n",
            "Epoch 2/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100 — Train Loss: 1.2109, Train Acc: 0.5021, Val Loss: 1.3510, Val Acc: 0.3684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/100 - Train: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s]\n",
            "Epoch 3/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/100 — Train Loss: 1.3753, Train Acc: 0.3354, Val Loss: 1.4038, Val Acc: 0.3158\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/100 - Train: 100%|██████████| 3/3 [00:03<00:00,  1.11s/it]\n",
            "Epoch 4/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/100 — Train Loss: 1.1224, Train Acc: 0.5458, Val Loss: 1.4827, Val Acc: 0.3158\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/100 - Train: 100%|██████████| 3/3 [00:02<00:00,  1.15it/s]\n",
            "Epoch 5/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100 — Train Loss: 1.0828, Train Acc: 0.5667, Val Loss: 1.5736, Val Acc: 0.2632\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/100 - Train: 100%|██████████| 3/3 [00:02<00:00,  1.14it/s]\n",
            "Epoch 6/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/100 — Train Loss: 1.1873, Train Acc: 0.4583, Val Loss: 1.7561, Val Acc: 0.4211\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/100 - Train: 100%|██████████| 3/3 [00:02<00:00,  1.14it/s]\n",
            "Epoch 7/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/100 — Train Loss: 1.0331, Train Acc: 0.5312, Val Loss: 1.9171, Val Acc: 0.3684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/100 - Train: 100%|██████████| 3/3 [00:03<00:00,  1.10s/it]\n",
            "Epoch 8/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/100 — Train Loss: 1.0520, Train Acc: 0.5063, Val Loss: 2.0482, Val Acc: 0.3684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/100 - Train: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s]\n",
            "Epoch 9/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/100 — Train Loss: 1.0394, Train Acc: 0.5208, Val Loss: 2.0665, Val Acc: 0.3684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/100 - Train: 100%|██████████| 3/3 [00:02<00:00,  1.19it/s]\n",
            "Epoch 10/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100 — Train Loss: 1.0073, Train Acc: 0.5833, Val Loss: 2.0130, Val Acc: 0.3684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/100 - Train: 100%|██████████| 3/3 [00:02<00:00,  1.19it/s]\n",
            "Epoch 11/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/100 — Train Loss: 0.9896, Train Acc: 0.7021, Val Loss: 1.8946, Val Acc: 0.3684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/100 - Train: 100%|██████████| 3/3 [00:03<00:00,  1.04s/it]\n",
            "Epoch 12/100 - Val: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/100 — Train Loss: 1.0202, Train Acc: 0.6708, Val Loss: 1.7846, Val Acc: 0.4211\n",
            "Early stopping triggered\n",
            "Fine-tuning complete. Best model saved to: results/fine_tuned_best_model.pth\n"
          ]
        }
      ],
      "source": [
        "# --- Fine-tuning with CutMix ---\n",
        "print(\"Starting Fine-tuning with CutMix...\")\n",
        "backbone = models.resnet50(weights=None)\n",
        "checkpoint = torch.load(os.path.join(results_dir, \"simclr_best_model.pth\"))\n",
        "backbone.load_state_dict(\n",
        "    {k: v for k, v in checkpoint['model_state_dict'].items() if 'backbone' in k},\n",
        "    strict=False\n",
        ")\n",
        "\n",
        "classifier = Classifier(backbone, num_classes=len(test_classes)).to(device)  # Assumes Classifier has dropout\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 100\n",
        "patience = 10\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    classifier.train()\n",
        "    total_train_loss = 0.0\n",
        "    total_train_acc = 0.0\n",
        "    for data, targets in tqdm(ft_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\"):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        if np.random.rand() < 0.5:  # 50% CutMix\n",
        "            data, (targets_a, targets_b, lam) = cutmix(data, targets)\n",
        "            logits = classifier(data)\n",
        "            loss = lam * criterion(logits, targets_a) + (1 - lam) * criterion(logits, targets_b)\n",
        "        else:\n",
        "            logits = classifier(data)\n",
        "            loss = criterion(logits, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        current_targets = targets_a if isinstance(targets, tuple) else targets\n",
        "        total_train_acc += (preds == current_targets).float().mean().item()\n",
        "    avg_train_loss = total_train_loss / len(ft_train_loader)\n",
        "    avg_train_acc = total_train_acc / len(ft_train_loader)\n",
        "\n",
        "    classifier.eval()\n",
        "    total_val_loss = 0.0\n",
        "    total_val_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in tqdm(ft_val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\"):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            logits = classifier(data)\n",
        "            loss = criterion(logits, targets)\n",
        "            total_val_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            total_val_acc += (preds == targets).float().mean().item()\n",
        "    avg_val_loss = total_val_loss / len(ft_val_loader)\n",
        "    avg_val_acc = total_val_acc / len(ft_val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} — Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}, \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    train_accs.append(avg_train_acc)\n",
        "    val_accs.append(avg_val_acc)\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': classifier.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_val_loss,\n",
        "            'acc': avg_val_acc\n",
        "        }, os.path.join(results_dir, \"fine_tuned_best_model.pth\"))\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Plot fine-tuning losses and accuracies\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Fine-tuning Losses')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(results_dir, \"finetune_losses.png\"))\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(val_accs, label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Fine-tuning Accuracies')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(results_dir, \"finetune_accs.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(f\"Fine-tuning complete. Best model saved to: {os.path.join(results_dir, 'fine_tuned_best_model.pth')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sct0P7aT_eao",
        "outputId": "1ac7b72e-0ad9-475b-cfcb-67d853358ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Evaluation...\n",
            "Evaluation Accuracy on Test Split: 0.2000\n"
          ]
        }
      ],
      "source": [
        "# --- Evaluation ---\n",
        "\n",
        "results_dir = \"results\"\n",
        "print(\"Starting Evaluation...\")\n",
        "\n",
        "# Load the saved checkpoint dict\n",
        "ckpt = torch.load(\n",
        "    os.path.join(results_dir, \"fine_tuned_bsest_model.pth\"),\n",
        "    map_location=device\n",
        ")\n",
        "\n",
        "# Load only the model weights\n",
        "classifier.load_state_dict(ckpt['model_state_dict'])\n",
        "classifier.to(device)\n",
        "classifier.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, targets in eval_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        logits = classifier(data)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "# Compute standard deviation of accuracy proportion\n",
        "p_hat = accuracy\n",
        "std = np.sqrt(p_hat * (1 - p_hat) / total)\n",
        "\n",
        "print(f\"Evaluation Accuracy on Test Split: {accuracy:.4f} ± {std:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHmyIr4m_FwF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FC1zPK9NamYN",
        "ZJ1-_8tG8YBt",
        "dDYFGE-JsMuT"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
